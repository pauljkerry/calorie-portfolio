{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7636798d-02c3-4e61-b732-00317f9f9f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "import optuna\n",
    "import importlib\n",
    "\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "\n",
    "import src.models.mlp.mlp_optuna_optimizer as op\n",
    "import src.models.mlp.mlp_cv_trainer as cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5a08409-763e-480d-ab9a-9844353171f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# データの読み込み\n",
    "env_path = Path.cwd().parent / \".env\"\n",
    "load_dotenv(dotenv_path=env_path)\n",
    "url = os.environ.get(\"OPTUNA_STORAGE_URL\")\n",
    "\n",
    "tr_df1 = pd.read_csv(\"../artifacts/features/tr_df1.csv\").astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bff0cb43-615a-4f9f-be31-e3c9d1064b1f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-23 01:24:03,462] A new study created in memory with name: mlp_v2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d2c2985eecb447b8ec45148a59f8ebf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/hanse/kaggle/calorie/src/models/mlp/mlp_cv_trainer.py:343: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(X_tr_tensor),\n",
      "/mnt/c/Users/hanse/kaggle/calorie/src/models/mlp/mlp_cv_trainer.py:344: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(y_tr_tensor),\n",
      "/mnt/c/Users/hanse/kaggle/calorie/src/models/mlp/mlp_cv_trainer.py:345: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(w_tr_tensor)\n",
      "/mnt/c/Users/hanse/kaggle/calorie/src/models/mlp/mlp_cv_trainer.py:348: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(X_val_tensor),\n",
      "/mnt/c/Users/hanse/kaggle/calorie/src/models/mlp/mlp_cv_trainer.py:349: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(y_val_tensor)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-07-23 01:24:03,853] Trial 0 failed with parameters: {'batch_size': 512, 'lr': 6.570000229733088e-05, 'dropout_rate': 0.23043342095233435, 'activation': 'ReLU', 'hidden_dims': '128, 64'} because of the following error: RuntimeError('Caught RuntimeError in DataLoader worker process 0.\\nOriginal Traceback (most recent call last):\\n  File \"/home/hanse/miniconda3/envs/torch22/lib/python3.11/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\\n    data = fetcher.fetch(index)\\n           ^^^^^^^^^^^^^^^^^^^^\\n  File \"/home/hanse/miniconda3/envs/torch22/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\\n    data = [self.dataset[idx] for idx in possibly_batched_index]\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/home/hanse/miniconda3/envs/torch22/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\\n    data = [self.dataset[idx] for idx in possibly_batched_index]\\n            ~~~~~~~~~~~~^^^^^\\n  File \"/home/hanse/miniconda3/envs/torch22/lib/python3.11/site-packages/torch/utils/data/dataset.py\", line 206, in __getitem__\\n    return tuple(tensor[index] for tensor in self.tensors)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/home/hanse/miniconda3/envs/torch22/lib/python3.11/site-packages/torch/utils/data/dataset.py\", line 206, in <genexpr>\\n    return tuple(tensor[index] for tensor in self.tensors)\\n                 ~~~~~~^^^^^^^\\nRuntimeError: CUDA error: initialization error\\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\\n\\n').\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/hanse/miniconda3/envs/torch22/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 201, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"/mnt/c/Users/hanse/kaggle/calorie/src/models/mlp/mlp_optuna_optimizer.py\", line 74, in objective\n",
      "    trainer.fit_one_fold(tr_df, fold=0)\n",
      "  File \"/mnt/c/Users/hanse/kaggle/calorie/src/models/mlp/mlp_cv_trainer.py\", line 382, in fit_one_fold\n",
      "    for xb, yb, wb in train_loader:\n",
      "  File \"/home/hanse/miniconda3/envs/torch22/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 631, in __next__\n",
      "    data = self._next_data()\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hanse/miniconda3/envs/torch22/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 1346, in _next_data\n",
      "    return self._process_data(data)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hanse/miniconda3/envs/torch22/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 1372, in _process_data\n",
      "    data.reraise()\n",
      "  File \"/home/hanse/miniconda3/envs/torch22/lib/python3.11/site-packages/torch/_utils.py\", line 722, in reraise\n",
      "    raise exception\n",
      "RuntimeError: Caught RuntimeError in DataLoader worker process 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/hanse/miniconda3/envs/torch22/lib/python3.11/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n",
      "    data = fetcher.fetch(index)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hanse/miniconda3/envs/torch22/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n",
      "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hanse/miniconda3/envs/torch22/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n",
      "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
      "            ~~~~~~~~~~~~^^^^^\n",
      "  File \"/home/hanse/miniconda3/envs/torch22/lib/python3.11/site-packages/torch/utils/data/dataset.py\", line 206, in __getitem__\n",
      "    return tuple(tensor[index] for tensor in self.tensors)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hanse/miniconda3/envs/torch22/lib/python3.11/site-packages/torch/utils/data/dataset.py\", line 206, in <genexpr>\n",
      "    return tuple(tensor[index] for tensor in self.tensors)\n",
      "                 ~~~~~~^^^^^^^\n",
      "RuntimeError: CUDA error: initialization error\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "\n",
      "[W 2025-07-23 01:24:03,872] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/hanse/miniconda3/envs/torch22/lib/python3.11/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/hanse/miniconda3/envs/torch22/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/hanse/miniconda3/envs/torch22/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n            ~~~~~~~~~~~~^^^^^\n  File \"/home/hanse/miniconda3/envs/torch22/lib/python3.11/site-packages/torch/utils/data/dataset.py\", line 206, in __getitem__\n    return tuple(tensor[index] for tensor in self.tensors)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/hanse/miniconda3/envs/torch22/lib/python3.11/site-packages/torch/utils/data/dataset.py\", line 206, in <genexpr>\n    return tuple(tensor[index] for tensor in self.tensors)\n                 ~~~~~~^^^^^^^\nRuntimeError: CUDA error: initialization error\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      4\u001b[39m objective = op.create_objective(tr_df1)\n\u001b[32m      6\u001b[39m random_sampler = optuna.samplers.RandomSampler(seed=\u001b[32m42\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m op.run_optuna_search(\n\u001b[32m      9\u001b[39m     objective,\n\u001b[32m     10\u001b[39m     n_trials=\u001b[32m1\u001b[39m,\n\u001b[32m     11\u001b[39m     study_name=\u001b[33m\"\u001b[39m\u001b[33mmlp_v2\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     12\u001b[39m     \u001b[38;5;66;03m# storage=url,\u001b[39;00m\n\u001b[32m     13\u001b[39m     \u001b[38;5;66;03m# sampler=random_sampler\u001b[39;00m\n\u001b[32m     14\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/c/Users/hanse/kaggle/calorie/src/models/mlp/mlp_optuna_optimizer.py:122\u001b[39m, in \u001b[36mrun_optuna_search\u001b[39m\u001b[34m(objective, n_trials, n_jobs, study_name, storage, initial_params, sampler)\u001b[39m\n\u001b[32m    119\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m initial_params \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    120\u001b[39m     study.enqueue_trial(initial_params)\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m study.optimize(\n\u001b[32m    123\u001b[39m     objective,\n\u001b[32m    124\u001b[39m     n_trials=n_trials,\n\u001b[32m    125\u001b[39m     n_jobs=n_jobs,\n\u001b[32m    126\u001b[39m     show_progress_bar=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    127\u001b[39m )\n\u001b[32m    129\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m study\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/torch22/lib/python3.11/site-packages/optuna/study/study.py:489\u001b[39m, in \u001b[36mStudy.optimize\u001b[39m\u001b[34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m    387\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34moptimize\u001b[39m(\n\u001b[32m    388\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    389\u001b[39m     func: ObjectiveFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m    396\u001b[39m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    397\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    398\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[32m    399\u001b[39m \n\u001b[32m    400\u001b[39m \u001b[33;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    487\u001b[39m \u001b[33;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[32m    488\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m489\u001b[39m     _optimize(\n\u001b[32m    490\u001b[39m         study=\u001b[38;5;28mself\u001b[39m,\n\u001b[32m    491\u001b[39m         func=func,\n\u001b[32m    492\u001b[39m         n_trials=n_trials,\n\u001b[32m    493\u001b[39m         timeout=timeout,\n\u001b[32m    494\u001b[39m         n_jobs=n_jobs,\n\u001b[32m    495\u001b[39m         catch=\u001b[38;5;28mtuple\u001b[39m(catch) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(catch, Iterable) \u001b[38;5;28;01melse\u001b[39;00m (catch,),\n\u001b[32m    496\u001b[39m         callbacks=callbacks,\n\u001b[32m    497\u001b[39m         gc_after_trial=gc_after_trial,\n\u001b[32m    498\u001b[39m         show_progress_bar=show_progress_bar,\n\u001b[32m    499\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/torch22/lib/python3.11/site-packages/optuna/study/_optimize.py:64\u001b[39m, in \u001b[36m_optimize\u001b[39m\u001b[34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     63\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs == \u001b[32m1\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m         _optimize_sequential(\n\u001b[32m     65\u001b[39m             study,\n\u001b[32m     66\u001b[39m             func,\n\u001b[32m     67\u001b[39m             n_trials,\n\u001b[32m     68\u001b[39m             timeout,\n\u001b[32m     69\u001b[39m             catch,\n\u001b[32m     70\u001b[39m             callbacks,\n\u001b[32m     71\u001b[39m             gc_after_trial,\n\u001b[32m     72\u001b[39m             reseed_sampler_rng=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     73\u001b[39m             time_start=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     74\u001b[39m             progress_bar=progress_bar,\n\u001b[32m     75\u001b[39m         )\n\u001b[32m     76\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     77\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs == -\u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/torch22/lib/python3.11/site-packages/optuna/study/_optimize.py:161\u001b[39m, in \u001b[36m_optimize_sequential\u001b[39m\u001b[34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[39m\n\u001b[32m    158\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    160\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m161\u001b[39m     frozen_trial = _run_trial(study, func, catch)\n\u001b[32m    162\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    163\u001b[39m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[32m    164\u001b[39m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[32m    165\u001b[39m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[32m    166\u001b[39m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[32m    167\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/torch22/lib/python3.11/site-packages/optuna/study/_optimize.py:253\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    246\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mShould not reach.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    248\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    249\u001b[39m     frozen_trial.state == TrialState.FAIL\n\u001b[32m    250\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    251\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[32m    252\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m253\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/torch22/lib/python3.11/site-packages/optuna/study/_optimize.py:201\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    199\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial._trial_id, study._storage):\n\u001b[32m    200\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m201\u001b[39m         value_or_values = func(trial)\n\u001b[32m    202\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions.TrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    203\u001b[39m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[32m    204\u001b[39m         state = TrialState.PRUNED\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/c/Users/hanse/kaggle/calorie/src/models/mlp/mlp_optuna_optimizer.py:74\u001b[39m, in \u001b[36mcreate_objective.<locals>.objective\u001b[39m\u001b[34m(trial)\u001b[39m\n\u001b[32m     66\u001b[39m hidden_dims_map = {\n\u001b[32m     67\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m128, 64\u001b[39m\u001b[33m\"\u001b[39m: [\u001b[32m128\u001b[39m, \u001b[32m64\u001b[39m],\n\u001b[32m     68\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m256, 128\u001b[39m\u001b[33m\"\u001b[39m: [\u001b[32m256\u001b[39m, \u001b[32m128\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m     71\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m512, 256, 128\u001b[39m\u001b[33m\"\u001b[39m: [\u001b[32m512\u001b[39m, \u001b[32m256\u001b[39m, \u001b[32m128\u001b[39m]\n\u001b[32m     72\u001b[39m }\n\u001b[32m     73\u001b[39m trainer.hidden_dims = hidden_dims_map[hidden_dims_key]\n\u001b[32m---> \u001b[39m\u001b[32m74\u001b[39m trainer.fit_one_fold(tr_df, fold=\u001b[32m0\u001b[39m)\n\u001b[32m     75\u001b[39m best_rounds = trainer.fold_models[\u001b[32m0\u001b[39m].best_rounds\n\u001b[32m     76\u001b[39m trial.set_user_attr(\u001b[33m\"\u001b[39m\u001b[33mbest_rounds\u001b[39m\u001b[33m\"\u001b[39m, best_rounds)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/c/Users/hanse/kaggle/calorie/src/models/mlp/mlp_cv_trainer.py:382\u001b[39m, in \u001b[36mMLPCVTrainer.fit_one_fold\u001b[39m\u001b[34m(self, tr_df, fold)\u001b[39m\n\u001b[32m    380\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.epochs):\n\u001b[32m    381\u001b[39m     model.train()\n\u001b[32m--> \u001b[39m\u001b[32m382\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m xb, yb, wb \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[32m    383\u001b[39m         optimizer.zero_grad()\n\u001b[32m    384\u001b[39m         \u001b[38;5;28;01mwith\u001b[39;00m autocast():  \u001b[38;5;66;03m# ← FP16でforward・loss計算\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/torch22/lib/python3.11/site-packages/torch/utils/data/dataloader.py:631\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    628\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    629\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    630\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m631\u001b[39m data = \u001b[38;5;28mself\u001b[39m._next_data()\n\u001b[32m    632\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    633\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[32m    634\u001b[39m         \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[32m    635\u001b[39m         \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/torch22/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1346\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1344\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1345\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m._task_info[idx]\n\u001b[32m-> \u001b[39m\u001b[32m1346\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._process_data(data)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/torch22/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1372\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._process_data\u001b[39m\u001b[34m(self, data)\u001b[39m\n\u001b[32m   1370\u001b[39m \u001b[38;5;28mself\u001b[39m._try_put_index()\n\u001b[32m   1371\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[32m-> \u001b[39m\u001b[32m1372\u001b[39m     data.reraise()\n\u001b[32m   1373\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/torch22/lib/python3.11/site-packages/torch/_utils.py:722\u001b[39m, in \u001b[36mExceptionWrapper.reraise\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    718\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m    719\u001b[39m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[32m    720\u001b[39m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[32m    721\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m722\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[31mRuntimeError\u001b[39m: Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/hanse/miniconda3/envs/torch22/lib/python3.11/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/hanse/miniconda3/envs/torch22/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/hanse/miniconda3/envs/torch22/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n            ~~~~~~~~~~~~^^^^^\n  File \"/home/hanse/miniconda3/envs/torch22/lib/python3.11/site-packages/torch/utils/data/dataset.py\", line 206, in __getitem__\n    return tuple(tensor[index] for tensor in self.tensors)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/hanse/miniconda3/envs/torch22/lib/python3.11/site-packages/torch/utils/data/dataset.py\", line 206, in <genexpr>\n    return tuple(tensor[index] for tensor in self.tensors)\n                 ~~~~~~^^^^^^^\nRuntimeError: CUDA error: initialization error\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n\n"
     ]
    }
   ],
   "source": [
    "# tr_df1\n",
    "importlib.reload(cv)\n",
    "importlib.reload(op)\n",
    "objective = op.create_objective(tr_df1)\n",
    "\n",
    "random_sampler = optuna.samplers.RandomSampler(seed=42)\n",
    "\n",
    "op.run_optuna_search(\n",
    "    objective,\n",
    "    n_trials=1,\n",
    "    study_name=\"mlp_v2\",\n",
    "    # storage=url,\n",
    "    # sampler=random_sampler\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7b7170-ada2-48e3-bfec-6039f0e01de0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-2.2.0",
   "language": "python",
   "name": "torch22"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
