{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9eb2fb4-1988-41de-88e3-2b07048a0062",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "import optuna\n",
    "\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "\n",
    "import src.models.lgbm.lgbm_optuna_optimizer as op\n",
    "import src.utils.telegram as te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1443cab-6c7f-48e7-950e-2317ccc99379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# データの読み込み\n",
    "env_path = Path.cwd().parent / \".env\"\n",
    "load_dotenv(dotenv_path=env_path)\n",
    "url = os.environ.get(\"OPTUNA_STORAGE_URL\")\n",
    "\n",
    "tr_df1 = pd.read_parquet(\"../artifacts/features/base/tr_df1.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac49c7d5-02bf-4f2f-b9a0-79a294a584e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-28 00:40:20,382] Using an existing study with name 'lgbm_v1' instead of creating a new one.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c488fd9bfc1544b89e51be5becb1d50a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 500 rounds\n",
      "[100]\ttrain's rmse: 0.19145\teval's rmse: 0.190982\n",
      "[200]\ttrain's rmse: 0.0884521\teval's rmse: 0.0878499\n",
      "[300]\ttrain's rmse: 0.0743551\teval's rmse: 0.073793\n",
      "[400]\ttrain's rmse: 0.0702942\teval's rmse: 0.0697694\n",
      "[500]\ttrain's rmse: 0.0684005\teval's rmse: 0.0679\n",
      "[600]\ttrain's rmse: 0.0668093\teval's rmse: 0.0663339\n",
      "[700]\ttrain's rmse: 0.0656805\teval's rmse: 0.0652202\n",
      "[800]\ttrain's rmse: 0.0649337\teval's rmse: 0.0644906\n",
      "[900]\ttrain's rmse: 0.0643852\teval's rmse: 0.063954\n",
      "[1000]\ttrain's rmse: 0.0640053\teval's rmse: 0.0635844\n",
      "[1100]\ttrain's rmse: 0.063667\teval's rmse: 0.0632562\n",
      "[1200]\ttrain's rmse: 0.0634121\teval's rmse: 0.0630115\n",
      "[1300]\ttrain's rmse: 0.0632239\teval's rmse: 0.0628316\n",
      "[1400]\ttrain's rmse: 0.0630608\teval's rmse: 0.0626749\n",
      "[1500]\ttrain's rmse: 0.0629229\teval's rmse: 0.0625463\n",
      "[1600]\ttrain's rmse: 0.0628019\teval's rmse: 0.0624342\n",
      "[1700]\ttrain's rmse: 0.0627069\teval's rmse: 0.0623472\n",
      "[1800]\ttrain's rmse: 0.062607\teval's rmse: 0.0622579\n",
      "[1900]\ttrain's rmse: 0.062528\teval's rmse: 0.0621896\n",
      "[2000]\ttrain's rmse: 0.0624585\teval's rmse: 0.0621271\n",
      "[2100]\ttrain's rmse: 0.0623928\teval's rmse: 0.0620691\n",
      "[2200]\ttrain's rmse: 0.0623223\teval's rmse: 0.0620076\n",
      "[2300]\ttrain's rmse: 0.0622696\teval's rmse: 0.061965\n",
      "[2400]\ttrain's rmse: 0.0622141\teval's rmse: 0.0619162\n",
      "[2500]\ttrain's rmse: 0.062159\teval's rmse: 0.0618701\n",
      "[2600]\ttrain's rmse: 0.0621074\teval's rmse: 0.0618272\n",
      "[2700]\ttrain's rmse: 0.0620584\teval's rmse: 0.0617876\n",
      "[2800]\ttrain's rmse: 0.0620146\teval's rmse: 0.0617523\n",
      "[2900]\ttrain's rmse: 0.0619767\teval's rmse: 0.0617207\n",
      "[3000]\ttrain's rmse: 0.0619384\teval's rmse: 0.0616899\n",
      "[3100]\ttrain's rmse: 0.0618942\teval's rmse: 0.0616543\n",
      "[3200]\ttrain's rmse: 0.0618569\teval's rmse: 0.0616238\n",
      "[3300]\ttrain's rmse: 0.0618218\teval's rmse: 0.0615959\n",
      "[3400]\ttrain's rmse: 0.0617811\teval's rmse: 0.0615619\n",
      "[3500]\ttrain's rmse: 0.0617474\teval's rmse: 0.0615343\n",
      "[3600]\ttrain's rmse: 0.0617168\teval's rmse: 0.0615099\n",
      "[3700]\ttrain's rmse: 0.0616849\teval's rmse: 0.0614849\n",
      "[3800]\ttrain's rmse: 0.0616572\teval's rmse: 0.061465\n",
      "[3900]\ttrain's rmse: 0.0616336\teval's rmse: 0.0614476\n",
      "[4000]\ttrain's rmse: 0.0616089\teval's rmse: 0.0614294\n",
      "[4100]\ttrain's rmse: 0.0615818\teval's rmse: 0.0614086\n",
      "[4200]\ttrain's rmse: 0.061556\teval's rmse: 0.0613893\n",
      "[4300]\ttrain's rmse: 0.0615285\teval's rmse: 0.0613692\n",
      "[4400]\ttrain's rmse: 0.0615041\teval's rmse: 0.0613517\n",
      "[4500]\ttrain's rmse: 0.0614822\teval's rmse: 0.0613347\n",
      "[4600]\ttrain's rmse: 0.0614556\teval's rmse: 0.061315\n",
      "[4700]\ttrain's rmse: 0.0614355\teval's rmse: 0.0613011\n",
      "[4800]\ttrain's rmse: 0.0614126\teval's rmse: 0.0612841\n",
      "[4900]\ttrain's rmse: 0.0613856\teval's rmse: 0.061263\n",
      "[5000]\ttrain's rmse: 0.0613633\teval's rmse: 0.0612458\n",
      "[5100]\ttrain's rmse: 0.0613388\teval's rmse: 0.0612278\n",
      "[5200]\ttrain's rmse: 0.0613218\teval's rmse: 0.0612178\n",
      "[5300]\ttrain's rmse: 0.061305\teval's rmse: 0.061206\n",
      "[5400]\ttrain's rmse: 0.0612825\teval's rmse: 0.0611901\n",
      "[5500]\ttrain's rmse: 0.0612609\teval's rmse: 0.0611744\n",
      "[5600]\ttrain's rmse: 0.061242\teval's rmse: 0.0611619\n",
      "[5700]\ttrain's rmse: 0.0612265\teval's rmse: 0.0611526\n",
      "[5800]\ttrain's rmse: 0.0612102\teval's rmse: 0.0611422\n",
      "[5900]\ttrain's rmse: 0.0611923\teval's rmse: 0.0611298\n",
      "[6000]\ttrain's rmse: 0.0611757\teval's rmse: 0.0611199\n",
      "[6100]\ttrain's rmse: 0.0611609\teval's rmse: 0.0611106\n",
      "[6200]\ttrain's rmse: 0.0611482\teval's rmse: 0.0611042\n",
      "[6300]\ttrain's rmse: 0.061128\teval's rmse: 0.0610908\n",
      "[6400]\ttrain's rmse: 0.0611135\teval's rmse: 0.0610819\n",
      "[6500]\ttrain's rmse: 0.0610951\teval's rmse: 0.0610696\n",
      "[6600]\ttrain's rmse: 0.0610778\teval's rmse: 0.0610579\n",
      "[6700]\ttrain's rmse: 0.061062\teval's rmse: 0.0610477\n",
      "[6800]\ttrain's rmse: 0.0610463\teval's rmse: 0.0610385\n",
      "[6900]\ttrain's rmse: 0.0610296\teval's rmse: 0.0610261\n",
      "[7000]\ttrain's rmse: 0.0610175\teval's rmse: 0.06102\n",
      "[7100]\ttrain's rmse: 0.0610023\teval's rmse: 0.0610082\n",
      "[7200]\ttrain's rmse: 0.0609898\teval's rmse: 0.0610014\n",
      "[7300]\ttrain's rmse: 0.0609769\teval's rmse: 0.0609939\n",
      "[7400]\ttrain's rmse: 0.0609645\teval's rmse: 0.0609867\n",
      "[7500]\ttrain's rmse: 0.0609488\teval's rmse: 0.0609755\n",
      "[7600]\ttrain's rmse: 0.060936\teval's rmse: 0.0609683\n",
      "[7700]\ttrain's rmse: 0.0609253\teval's rmse: 0.060963\n",
      "[7800]\ttrain's rmse: 0.0609122\teval's rmse: 0.0609547\n",
      "[7900]\ttrain's rmse: 0.0608986\teval's rmse: 0.060947\n",
      "[8000]\ttrain's rmse: 0.0608865\teval's rmse: 0.0609398\n",
      "[8100]\ttrain's rmse: 0.0608741\teval's rmse: 0.0609323\n",
      "[8200]\ttrain's rmse: 0.0608632\teval's rmse: 0.0609264\n",
      "[8300]\ttrain's rmse: 0.0608511\teval's rmse: 0.0609193\n",
      "[8400]\ttrain's rmse: 0.0608408\teval's rmse: 0.0609131\n",
      "[8500]\ttrain's rmse: 0.0608286\teval's rmse: 0.0609067\n",
      "[8600]\ttrain's rmse: 0.0608174\teval's rmse: 0.0609018\n",
      "[8700]\ttrain's rmse: 0.0608075\teval's rmse: 0.0608963\n",
      "[8800]\ttrain's rmse: 0.0607972\teval's rmse: 0.0608913\n",
      "[8900]\ttrain's rmse: 0.0607863\teval's rmse: 0.0608865\n",
      "[9000]\ttrain's rmse: 0.0607749\teval's rmse: 0.0608795\n",
      "[9100]\ttrain's rmse: 0.0607634\teval's rmse: 0.0608738\n",
      "[9200]\ttrain's rmse: 0.0607534\teval's rmse: 0.0608677\n",
      "[9300]\ttrain's rmse: 0.0607423\teval's rmse: 0.060861\n",
      "[9400]\ttrain's rmse: 0.0607331\teval's rmse: 0.0608554\n",
      "[9500]\ttrain's rmse: 0.0607225\teval's rmse: 0.0608499\n",
      "[9600]\ttrain's rmse: 0.0607118\teval's rmse: 0.060843\n",
      "[9700]\ttrain's rmse: 0.0607006\teval's rmse: 0.0608373\n",
      "[9800]\ttrain's rmse: 0.0606914\teval's rmse: 0.0608323\n",
      "[9900]\ttrain's rmse: 0.0606824\teval's rmse: 0.0608268\n",
      "[10000]\ttrain's rmse: 0.0606722\teval's rmse: 0.0608225\n",
      "[10100]\ttrain's rmse: 0.0606632\teval's rmse: 0.060818\n",
      "[10200]\ttrain's rmse: 0.0606529\teval's rmse: 0.0608126\n",
      "[10300]\ttrain's rmse: 0.0606428\teval's rmse: 0.0608067\n",
      "[10400]\ttrain's rmse: 0.0606334\teval's rmse: 0.0608017\n",
      "[10500]\ttrain's rmse: 0.0606252\teval's rmse: 0.0607979\n",
      "[10600]\ttrain's rmse: 0.0606158\teval's rmse: 0.0607931\n",
      "[10700]\ttrain's rmse: 0.0606068\teval's rmse: 0.0607888\n",
      "[10800]\ttrain's rmse: 0.060598\teval's rmse: 0.0607846\n",
      "[10900]\ttrain's rmse: 0.0605908\teval's rmse: 0.0607809\n",
      "[11000]\ttrain's rmse: 0.0605826\teval's rmse: 0.0607774\n",
      "[11100]\ttrain's rmse: 0.0605738\teval's rmse: 0.0607732\n",
      "[11200]\ttrain's rmse: 0.0605662\teval's rmse: 0.0607697\n",
      "[11300]\ttrain's rmse: 0.0605578\teval's rmse: 0.060765\n",
      "[11400]\ttrain's rmse: 0.0605486\teval's rmse: 0.0607607\n",
      "[11500]\ttrain's rmse: 0.0605419\teval's rmse: 0.0607571\n",
      "[11600]\ttrain's rmse: 0.0605345\teval's rmse: 0.0607531\n",
      "[11700]\ttrain's rmse: 0.0605256\teval's rmse: 0.0607483\n",
      "[11800]\ttrain's rmse: 0.0605182\teval's rmse: 0.0607453\n",
      "[11900]\ttrain's rmse: 0.0605107\teval's rmse: 0.0607413\n",
      "[12000]\ttrain's rmse: 0.0605028\teval's rmse: 0.0607387\n",
      "[12100]\ttrain's rmse: 0.0604941\teval's rmse: 0.0607341\n",
      "[12200]\ttrain's rmse: 0.0604865\teval's rmse: 0.0607304\n",
      "[12300]\ttrain's rmse: 0.0604793\teval's rmse: 0.0607278\n",
      "[12400]\ttrain's rmse: 0.0604711\teval's rmse: 0.0607235\n",
      "[12500]\ttrain's rmse: 0.0604638\teval's rmse: 0.0607205\n",
      "[12600]\ttrain's rmse: 0.0604558\teval's rmse: 0.0607165\n",
      "[12700]\ttrain's rmse: 0.0604486\teval's rmse: 0.0607132\n",
      "[12800]\ttrain's rmse: 0.0604415\teval's rmse: 0.0607105\n",
      "[12900]\ttrain's rmse: 0.0604349\teval's rmse: 0.0607077\n",
      "[13000]\ttrain's rmse: 0.0604286\teval's rmse: 0.0607053\n",
      "[13100]\ttrain's rmse: 0.0604215\teval's rmse: 0.0607022\n",
      "[13200]\ttrain's rmse: 0.0604153\teval's rmse: 0.0606995\n",
      "[13300]\ttrain's rmse: 0.0604088\teval's rmse: 0.0606973\n",
      "[13400]\ttrain's rmse: 0.0604028\teval's rmse: 0.0606943\n",
      "[13500]\ttrain's rmse: 0.0603954\teval's rmse: 0.0606908\n",
      "[13600]\ttrain's rmse: 0.0603878\teval's rmse: 0.0606873\n",
      "[13700]\ttrain's rmse: 0.0603807\teval's rmse: 0.060685\n",
      "[13800]\ttrain's rmse: 0.0603739\teval's rmse: 0.0606809\n",
      "[13900]\ttrain's rmse: 0.0603674\teval's rmse: 0.0606783\n",
      "[14000]\ttrain's rmse: 0.06036\teval's rmse: 0.0606755\n",
      "[14100]\ttrain's rmse: 0.0603541\teval's rmse: 0.0606737\n",
      "[14200]\ttrain's rmse: 0.0603488\teval's rmse: 0.0606718\n",
      "[14300]\ttrain's rmse: 0.0603425\teval's rmse: 0.0606709\n",
      "[14400]\ttrain's rmse: 0.060336\teval's rmse: 0.0606668\n",
      "[14500]\ttrain's rmse: 0.0603303\teval's rmse: 0.0606651\n",
      "[14600]\ttrain's rmse: 0.0603248\teval's rmse: 0.0606633\n",
      "[14700]\ttrain's rmse: 0.060319\teval's rmse: 0.0606607\n",
      "[14800]\ttrain's rmse: 0.0603123\teval's rmse: 0.0606587\n",
      "[14900]\ttrain's rmse: 0.0603063\teval's rmse: 0.0606554\n",
      "[15000]\ttrain's rmse: 0.0603004\teval's rmse: 0.0606535\n",
      "[15100]\ttrain's rmse: 0.0602942\teval's rmse: 0.0606515\n",
      "[15200]\ttrain's rmse: 0.0602881\teval's rmse: 0.0606493\n",
      "[15300]\ttrain's rmse: 0.0602821\teval's rmse: 0.0606468\n",
      "[15400]\ttrain's rmse: 0.0602761\teval's rmse: 0.0606445\n",
      "[15500]\ttrain's rmse: 0.0602697\teval's rmse: 0.0606415\n",
      "[15600]\ttrain's rmse: 0.0602649\teval's rmse: 0.0606401\n",
      "[15700]\ttrain's rmse: 0.0602586\teval's rmse: 0.0606379\n",
      "[15800]\ttrain's rmse: 0.0602524\teval's rmse: 0.0606347\n",
      "[15900]\ttrain's rmse: 0.0602463\teval's rmse: 0.0606338\n",
      "[16000]\ttrain's rmse: 0.0602417\teval's rmse: 0.060633\n",
      "[16100]\ttrain's rmse: 0.0602357\teval's rmse: 0.0606304\n",
      "[16200]\ttrain's rmse: 0.0602303\teval's rmse: 0.0606281\n",
      "[16300]\ttrain's rmse: 0.0602248\teval's rmse: 0.0606258\n",
      "[16400]\ttrain's rmse: 0.0602186\teval's rmse: 0.0606249\n",
      "[16500]\ttrain's rmse: 0.0602128\teval's rmse: 0.0606221\n",
      "[16600]\ttrain's rmse: 0.0602082\teval's rmse: 0.0606214\n",
      "[16700]\ttrain's rmse: 0.0602031\teval's rmse: 0.0606197\n",
      "[16800]\ttrain's rmse: 0.0601981\teval's rmse: 0.0606181\n",
      "[16900]\ttrain's rmse: 0.0601925\teval's rmse: 0.0606155\n",
      "[17000]\ttrain's rmse: 0.0601871\teval's rmse: 0.060613\n",
      "[17100]\ttrain's rmse: 0.0601819\teval's rmse: 0.0606122\n",
      "[17200]\ttrain's rmse: 0.0601766\teval's rmse: 0.0606111\n",
      "[17300]\ttrain's rmse: 0.0601718\teval's rmse: 0.0606094\n",
      "[17400]\ttrain's rmse: 0.0601671\teval's rmse: 0.0606078\n",
      "[17500]\ttrain's rmse: 0.060162\teval's rmse: 0.0606071\n",
      "[17600]\ttrain's rmse: 0.0601571\teval's rmse: 0.0606057\n",
      "[17700]\ttrain's rmse: 0.0601523\teval's rmse: 0.0606042\n",
      "[17800]\ttrain's rmse: 0.0601467\teval's rmse: 0.0606026\n",
      "[17900]\ttrain's rmse: 0.0601414\teval's rmse: 0.0606008\n",
      "[18000]\ttrain's rmse: 0.0601361\teval's rmse: 0.060599\n",
      "[18100]\ttrain's rmse: 0.0601314\teval's rmse: 0.0605982\n",
      "[18200]\ttrain's rmse: 0.0601261\teval's rmse: 0.0605959\n",
      "[18300]\ttrain's rmse: 0.0601214\teval's rmse: 0.0605941\n",
      "[18400]\ttrain's rmse: 0.0601166\teval's rmse: 0.0605923\n",
      "[18500]\ttrain's rmse: 0.0601119\teval's rmse: 0.0605903\n",
      "[18600]\ttrain's rmse: 0.0601074\teval's rmse: 0.0605897\n",
      "[18700]\ttrain's rmse: 0.0601024\teval's rmse: 0.0605878\n",
      "[18800]\ttrain's rmse: 0.0600981\teval's rmse: 0.0605862\n",
      "[18900]\ttrain's rmse: 0.060093\teval's rmse: 0.0605856\n",
      "[19000]\ttrain's rmse: 0.0600884\teval's rmse: 0.0605839\n",
      "[19100]\ttrain's rmse: 0.0600839\teval's rmse: 0.0605818\n",
      "[19200]\ttrain's rmse: 0.0600796\teval's rmse: 0.0605811\n",
      "[19300]\ttrain's rmse: 0.0600748\teval's rmse: 0.0605793\n",
      "[19400]\ttrain's rmse: 0.0600705\teval's rmse: 0.0605785\n",
      "[19500]\ttrain's rmse: 0.0600659\teval's rmse: 0.0605762\n",
      "[19600]\ttrain's rmse: 0.0600615\teval's rmse: 0.0605746\n",
      "[19700]\ttrain's rmse: 0.0600565\teval's rmse: 0.060574\n",
      "[19800]\ttrain's rmse: 0.0600518\teval's rmse: 0.0605732\n",
      "[19900]\ttrain's rmse: 0.0600469\teval's rmse: 0.0605715\n",
      "[20000]\ttrain's rmse: 0.0600423\teval's rmse: 0.06057\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[20000]\ttrain's rmse: 0.0600423\teval's rmse: 0.06057\n",
      "Training time: 00:38:02\n",
      "Train rmse: 0.06004\n",
      "Valid rmse: 0.06057\n",
      "[I 2025-07-28 01:18:23,395] Trial 10 finished with value: 0.060569968168224915 and parameters: {'learning_rate': 0.02, 'max_depth': 5, 'num_leaves': 523, 'min_child_samples': 5056, 'min_split_gain': 9.275294835907687e-05, 'feature_fraction': 0.43593953046851464, 'bagging_fraction': 0.8924361138693251, 'bagging_freq': 10, 'lambda_l1': 1.6934155410667961, 'lambda_l2': 0.6637926838138378}. Best is trial 0 with value: 0.060481305339152816.\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[100]\ttrain's rmse: 0.246555\teval's rmse: 0.245831\n",
      "[200]\ttrain's rmse: 0.146533\teval's rmse: 0.145624\n",
      "[300]\ttrain's rmse: 0.124856\teval's rmse: 0.123996\n",
      "[400]\ttrain's rmse: 0.112163\teval's rmse: 0.111363\n",
      "[500]\ttrain's rmse: 0.100768\teval's rmse: 0.100023\n",
      "[600]\ttrain's rmse: 0.0953427\teval's rmse: 0.0946419\n",
      "[700]\ttrain's rmse: 0.089804\teval's rmse: 0.0891365\n",
      "[800]\ttrain's rmse: 0.0862475\teval's rmse: 0.0856031\n",
      "[900]\ttrain's rmse: 0.0837117\teval's rmse: 0.0830861\n",
      "[1000]\ttrain's rmse: 0.0811716\teval's rmse: 0.0805611\n",
      "[1100]\ttrain's rmse: 0.0797704\teval's rmse: 0.0791671\n",
      "[1200]\ttrain's rmse: 0.0787558\teval's rmse: 0.0781584\n",
      "[1300]\ttrain's rmse: 0.0777927\teval's rmse: 0.0772038\n",
      "[1400]\ttrain's rmse: 0.0772002\teval's rmse: 0.0766153\n",
      "[1500]\ttrain's rmse: 0.076709\teval's rmse: 0.0761311\n",
      "[1600]\ttrain's rmse: 0.0763309\teval's rmse: 0.0757551\n",
      "[1700]\ttrain's rmse: 0.0759888\teval's rmse: 0.0754162\n",
      "[1800]\ttrain's rmse: 0.0757575\teval's rmse: 0.0751854\n",
      "[1900]\ttrain's rmse: 0.075514\teval's rmse: 0.0749482\n",
      "[2000]\ttrain's rmse: 0.0753804\teval's rmse: 0.0748148\n",
      "[2100]\ttrain's rmse: 0.0752462\teval's rmse: 0.0746825\n",
      "[2200]\ttrain's rmse: 0.0751633\teval's rmse: 0.0746016\n",
      "[2300]\ttrain's rmse: 0.0750898\teval's rmse: 0.0745288\n",
      "[2400]\ttrain's rmse: 0.0750124\teval's rmse: 0.0744543\n",
      "[2500]\ttrain's rmse: 0.0749291\teval's rmse: 0.0743719\n",
      "[2600]\ttrain's rmse: 0.0748798\teval's rmse: 0.0743247\n",
      "[2700]\ttrain's rmse: 0.0748265\teval's rmse: 0.0742717\n",
      "[2800]\ttrain's rmse: 0.0747891\teval's rmse: 0.0742359\n",
      "[2900]\ttrain's rmse: 0.0747709\teval's rmse: 0.0742169\n",
      "[3000]\ttrain's rmse: 0.0747462\teval's rmse: 0.0741934\n",
      "[3100]\ttrain's rmse: 0.0747232\teval's rmse: 0.0741718\n",
      "[3200]\ttrain's rmse: 0.0747072\teval's rmse: 0.0741571\n",
      "[3300]\ttrain's rmse: 0.0746919\teval's rmse: 0.0741408\n",
      "[3400]\ttrain's rmse: 0.0746766\teval's rmse: 0.0741268\n",
      "[3500]\ttrain's rmse: 0.0746644\teval's rmse: 0.0741147\n",
      "[3600]\ttrain's rmse: 0.0746476\teval's rmse: 0.0740989\n",
      "[3700]\ttrain's rmse: 0.0746381\teval's rmse: 0.0740904\n",
      "[3800]\ttrain's rmse: 0.0746271\teval's rmse: 0.0740795\n",
      "[3900]\ttrain's rmse: 0.0746181\teval's rmse: 0.0740707\n",
      "[4000]\ttrain's rmse: 0.074611\teval's rmse: 0.0740639\n",
      "[4100]\ttrain's rmse: 0.0746013\teval's rmse: 0.074055\n",
      "[4200]\ttrain's rmse: 0.0745902\teval's rmse: 0.0740444\n",
      "[4300]\ttrain's rmse: 0.0745838\teval's rmse: 0.0740376\n",
      "[4400]\ttrain's rmse: 0.0745722\teval's rmse: 0.0740272\n",
      "[4500]\ttrain's rmse: 0.074567\teval's rmse: 0.0740224\n",
      "[4600]\ttrain's rmse: 0.0745638\teval's rmse: 0.0740192\n",
      "[4700]\ttrain's rmse: 0.0745585\teval's rmse: 0.0740142\n",
      "[4800]\ttrain's rmse: 0.0745534\teval's rmse: 0.0740085\n",
      "[4900]\ttrain's rmse: 0.0745399\teval's rmse: 0.0739952\n",
      "[5000]\ttrain's rmse: 0.0745354\teval's rmse: 0.0739916\n",
      "[5100]\ttrain's rmse: 0.0745319\teval's rmse: 0.0739886\n",
      "[5200]\ttrain's rmse: 0.0745287\teval's rmse: 0.0739854\n",
      "[5300]\ttrain's rmse: 0.0745234\teval's rmse: 0.0739804\n",
      "[5400]\ttrain's rmse: 0.0745209\teval's rmse: 0.0739776\n",
      "[5500]\ttrain's rmse: 0.0745173\teval's rmse: 0.0739737\n",
      "[5600]\ttrain's rmse: 0.0745146\teval's rmse: 0.0739712\n",
      "[5700]\ttrain's rmse: 0.0745129\teval's rmse: 0.0739694\n",
      "[5800]\ttrain's rmse: 0.0745111\teval's rmse: 0.0739676\n",
      "[5900]\ttrain's rmse: 0.074509\teval's rmse: 0.0739659\n",
      "[6000]\ttrain's rmse: 0.074507\teval's rmse: 0.0739641\n",
      "[6100]\ttrain's rmse: 0.0745031\teval's rmse: 0.0739604\n",
      "[6200]\ttrain's rmse: 0.0745007\teval's rmse: 0.073958\n",
      "[6300]\ttrain's rmse: 0.0744981\teval's rmse: 0.0739556\n",
      "[6400]\ttrain's rmse: 0.0744969\teval's rmse: 0.0739544\n",
      "[6500]\ttrain's rmse: 0.0744957\teval's rmse: 0.0739528\n",
      "[6600]\ttrain's rmse: 0.0744944\teval's rmse: 0.0739518\n",
      "[6700]\ttrain's rmse: 0.0744932\teval's rmse: 0.0739504\n",
      "[6800]\ttrain's rmse: 0.0744923\teval's rmse: 0.0739496\n",
      "[6900]\ttrain's rmse: 0.0744907\teval's rmse: 0.073948\n",
      "[7000]\ttrain's rmse: 0.0744898\teval's rmse: 0.0739473\n",
      "[7100]\ttrain's rmse: 0.0744882\teval's rmse: 0.0739463\n",
      "[7200]\ttrain's rmse: 0.0744869\teval's rmse: 0.0739452\n",
      "[7300]\ttrain's rmse: 0.0744858\teval's rmse: 0.0739441\n",
      "[7400]\ttrain's rmse: 0.0744851\teval's rmse: 0.0739432\n",
      "[7500]\ttrain's rmse: 0.0744838\teval's rmse: 0.0739417\n",
      "[7600]\ttrain's rmse: 0.0744829\teval's rmse: 0.0739409\n",
      "[7700]\ttrain's rmse: 0.0744824\teval's rmse: 0.0739404\n",
      "[7800]\ttrain's rmse: 0.0744816\teval's rmse: 0.0739397\n",
      "[7900]\ttrain's rmse: 0.0744808\teval's rmse: 0.0739389\n",
      "[8000]\ttrain's rmse: 0.0744794\teval's rmse: 0.0739377\n",
      "[8100]\ttrain's rmse: 0.0744787\teval's rmse: 0.0739367\n",
      "[8200]\ttrain's rmse: 0.0744783\teval's rmse: 0.0739363\n",
      "[8300]\ttrain's rmse: 0.0744774\teval's rmse: 0.0739356\n",
      "[8400]\ttrain's rmse: 0.074477\teval's rmse: 0.0739354\n",
      "[8500]\ttrain's rmse: 0.0744761\teval's rmse: 0.0739344\n",
      "[8600]\ttrain's rmse: 0.074475\teval's rmse: 0.0739336\n",
      "[8700]\ttrain's rmse: 0.0744738\teval's rmse: 0.0739325\n",
      "[8800]\ttrain's rmse: 0.0744732\teval's rmse: 0.073932\n",
      "[8900]\ttrain's rmse: 0.0744727\teval's rmse: 0.0739314\n",
      "[9000]\ttrain's rmse: 0.0744722\teval's rmse: 0.0739308\n",
      "[9100]\ttrain's rmse: 0.0744718\teval's rmse: 0.0739305\n",
      "[9200]\ttrain's rmse: 0.0744711\teval's rmse: 0.0739297\n",
      "[9300]\ttrain's rmse: 0.0744707\teval's rmse: 0.0739295\n",
      "[9400]\ttrain's rmse: 0.0744701\teval's rmse: 0.0739291\n",
      "[9500]\ttrain's rmse: 0.074469\teval's rmse: 0.0739287\n",
      "[9600]\ttrain's rmse: 0.0744686\teval's rmse: 0.073928\n",
      "[9700]\ttrain's rmse: 0.0744674\teval's rmse: 0.073927\n",
      "[9800]\ttrain's rmse: 0.0744669\teval's rmse: 0.0739266\n",
      "[9900]\ttrain's rmse: 0.0744664\teval's rmse: 0.073926\n",
      "[10000]\ttrain's rmse: 0.0744661\teval's rmse: 0.0739255\n",
      "[10100]\ttrain's rmse: 0.0744656\teval's rmse: 0.0739249\n",
      "[10200]\ttrain's rmse: 0.0744652\teval's rmse: 0.0739246\n",
      "[10300]\ttrain's rmse: 0.0744649\teval's rmse: 0.0739245\n",
      "[10400]\ttrain's rmse: 0.0744644\teval's rmse: 0.0739239\n",
      "[10500]\ttrain's rmse: 0.0744641\teval's rmse: 0.0739239\n",
      "[10600]\ttrain's rmse: 0.0744635\teval's rmse: 0.0739232\n",
      "[10700]\ttrain's rmse: 0.0744633\teval's rmse: 0.0739231\n",
      "[10800]\ttrain's rmse: 0.074463\teval's rmse: 0.0739228\n",
      "[10900]\ttrain's rmse: 0.0744626\teval's rmse: 0.0739225\n",
      "[11000]\ttrain's rmse: 0.0744619\teval's rmse: 0.0739219\n",
      "[11100]\ttrain's rmse: 0.0744613\teval's rmse: 0.0739214\n",
      "[11200]\ttrain's rmse: 0.0744605\teval's rmse: 0.073921\n",
      "[11300]\ttrain's rmse: 0.0744602\teval's rmse: 0.0739206\n",
      "[11400]\ttrain's rmse: 0.0744599\teval's rmse: 0.0739206\n",
      "[11500]\ttrain's rmse: 0.0744597\teval's rmse: 0.0739202\n",
      "[11600]\ttrain's rmse: 0.0744585\teval's rmse: 0.0739193\n",
      "[11700]\ttrain's rmse: 0.0744579\teval's rmse: 0.0739187\n",
      "[11800]\ttrain's rmse: 0.0744576\teval's rmse: 0.0739185\n",
      "[11900]\ttrain's rmse: 0.0744573\teval's rmse: 0.0739183\n",
      "[12000]\ttrain's rmse: 0.0744568\teval's rmse: 0.0739182\n",
      "[12100]\ttrain's rmse: 0.0744566\teval's rmse: 0.0739181\n",
      "[12200]\ttrain's rmse: 0.0744564\teval's rmse: 0.0739177\n",
      "[12300]\ttrain's rmse: 0.0744561\teval's rmse: 0.0739173\n",
      "[12400]\ttrain's rmse: 0.0744559\teval's rmse: 0.0739169\n",
      "[12500]\ttrain's rmse: 0.0744553\teval's rmse: 0.0739166\n",
      "[12600]\ttrain's rmse: 0.074455\teval's rmse: 0.0739165\n",
      "[12700]\ttrain's rmse: 0.0744546\teval's rmse: 0.0739161\n",
      "[12800]\ttrain's rmse: 0.0744544\teval's rmse: 0.0739162\n",
      "[12900]\ttrain's rmse: 0.074454\teval's rmse: 0.0739156\n",
      "[13000]\ttrain's rmse: 0.0744538\teval's rmse: 0.0739153\n",
      "[13100]\ttrain's rmse: 0.0744535\teval's rmse: 0.0739149\n",
      "[13200]\ttrain's rmse: 0.0744534\teval's rmse: 0.0739149\n",
      "[13300]\ttrain's rmse: 0.0744531\teval's rmse: 0.0739147\n",
      "[13400]\ttrain's rmse: 0.0744529\teval's rmse: 0.0739145\n",
      "[13500]\ttrain's rmse: 0.0744527\teval's rmse: 0.0739143\n",
      "[13600]\ttrain's rmse: 0.0744525\teval's rmse: 0.0739143\n",
      "[13700]\ttrain's rmse: 0.0744523\teval's rmse: 0.0739141\n",
      "[13800]\ttrain's rmse: 0.0744522\teval's rmse: 0.073914\n",
      "[13900]\ttrain's rmse: 0.074452\teval's rmse: 0.0739138\n",
      "[14000]\ttrain's rmse: 0.0744516\teval's rmse: 0.0739132\n",
      "[14100]\ttrain's rmse: 0.0744515\teval's rmse: 0.0739132\n",
      "[14200]\ttrain's rmse: 0.0744513\teval's rmse: 0.0739128\n",
      "[14300]\ttrain's rmse: 0.0744511\teval's rmse: 0.0739128\n",
      "[14400]\ttrain's rmse: 0.074451\teval's rmse: 0.0739127\n",
      "[14500]\ttrain's rmse: 0.0744509\teval's rmse: 0.0739125\n",
      "[14600]\ttrain's rmse: 0.0744505\teval's rmse: 0.0739123\n",
      "[14700]\ttrain's rmse: 0.0744503\teval's rmse: 0.0739123\n",
      "[14800]\ttrain's rmse: 0.0744501\teval's rmse: 0.0739122\n",
      "[14900]\ttrain's rmse: 0.07445\teval's rmse: 0.073912\n",
      "[15000]\ttrain's rmse: 0.0744497\teval's rmse: 0.0739116\n",
      "[15100]\ttrain's rmse: 0.0744496\teval's rmse: 0.0739115\n",
      "[15200]\ttrain's rmse: 0.0744494\teval's rmse: 0.0739113\n",
      "[15300]\ttrain's rmse: 0.0744492\teval's rmse: 0.0739113\n",
      "[15400]\ttrain's rmse: 0.0744491\teval's rmse: 0.0739112\n",
      "[15500]\ttrain's rmse: 0.0744489\teval's rmse: 0.073911\n",
      "[15600]\ttrain's rmse: 0.0744488\teval's rmse: 0.0739109\n",
      "[15700]\ttrain's rmse: 0.0744482\teval's rmse: 0.0739103\n",
      "[15800]\ttrain's rmse: 0.0744481\teval's rmse: 0.0739101\n",
      "[15900]\ttrain's rmse: 0.0744479\teval's rmse: 0.0739098\n",
      "[16000]\ttrain's rmse: 0.0744477\teval's rmse: 0.0739096\n",
      "[16100]\ttrain's rmse: 0.0744476\teval's rmse: 0.0739095\n",
      "[16200]\ttrain's rmse: 0.0744474\teval's rmse: 0.0739093\n",
      "[16300]\ttrain's rmse: 0.0744469\teval's rmse: 0.0739085\n",
      "[16400]\ttrain's rmse: 0.0744466\teval's rmse: 0.0739084\n",
      "[16500]\ttrain's rmse: 0.0744463\teval's rmse: 0.073908\n",
      "[16600]\ttrain's rmse: 0.0744462\teval's rmse: 0.073908\n",
      "[16700]\ttrain's rmse: 0.0744461\teval's rmse: 0.0739078\n",
      "[16800]\ttrain's rmse: 0.074446\teval's rmse: 0.0739079\n",
      "[16900]\ttrain's rmse: 0.0744458\teval's rmse: 0.0739076\n",
      "[17000]\ttrain's rmse: 0.0744457\teval's rmse: 0.0739075\n",
      "[17100]\ttrain's rmse: 0.0744456\teval's rmse: 0.0739075\n",
      "[17200]\ttrain's rmse: 0.0744455\teval's rmse: 0.0739073\n",
      "[17300]\ttrain's rmse: 0.0744452\teval's rmse: 0.0739071\n",
      "[17400]\ttrain's rmse: 0.0744451\teval's rmse: 0.0739069\n",
      "[17500]\ttrain's rmse: 0.0744451\teval's rmse: 0.0739068\n",
      "[17600]\ttrain's rmse: 0.0744449\teval's rmse: 0.0739066\n",
      "[17700]\ttrain's rmse: 0.0744448\teval's rmse: 0.0739065\n",
      "[17800]\ttrain's rmse: 0.0744445\teval's rmse: 0.0739062\n",
      "[17900]\ttrain's rmse: 0.0744444\teval's rmse: 0.073906\n",
      "[18000]\ttrain's rmse: 0.0744444\teval's rmse: 0.0739059\n",
      "[18100]\ttrain's rmse: 0.0744442\teval's rmse: 0.0739059\n",
      "[18200]\ttrain's rmse: 0.0744441\teval's rmse: 0.0739058\n",
      "[18300]\ttrain's rmse: 0.0744439\teval's rmse: 0.0739057\n",
      "[18400]\ttrain's rmse: 0.0744437\teval's rmse: 0.0739055\n",
      "[18500]\ttrain's rmse: 0.0744436\teval's rmse: 0.0739054\n",
      "[18600]\ttrain's rmse: 0.0744434\teval's rmse: 0.0739052\n",
      "[18700]\ttrain's rmse: 0.0744432\teval's rmse: 0.0739048\n",
      "[18800]\ttrain's rmse: 0.074443\teval's rmse: 0.0739045\n",
      "[18900]\ttrain's rmse: 0.0744429\teval's rmse: 0.0739046\n",
      "[19000]\ttrain's rmse: 0.0744428\teval's rmse: 0.0739044\n",
      "[19100]\ttrain's rmse: 0.0744427\teval's rmse: 0.0739041\n",
      "[19200]\ttrain's rmse: 0.0744426\teval's rmse: 0.0739042\n",
      "[19300]\ttrain's rmse: 0.0744426\teval's rmse: 0.0739042\n",
      "[19400]\ttrain's rmse: 0.0744422\teval's rmse: 0.073904\n",
      "[19500]\ttrain's rmse: 0.0744421\teval's rmse: 0.0739041\n",
      "[19600]\ttrain's rmse: 0.074442\teval's rmse: 0.0739038\n",
      "[19700]\ttrain's rmse: 0.0744419\teval's rmse: 0.0739038\n",
      "[19800]\ttrain's rmse: 0.0744416\teval's rmse: 0.0739034\n",
      "[19900]\ttrain's rmse: 0.0744415\teval's rmse: 0.0739036\n",
      "[20000]\ttrain's rmse: 0.0744414\teval's rmse: 0.0739035\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[19796]\ttrain's rmse: 0.0744416\teval's rmse: 0.0739034\n",
      "Training time: 00:04:12\n",
      "Train rmse: 0.07444\n",
      "Valid rmse: 0.07390\n",
      "[I 2025-07-28 01:22:36,690] Trial 11 finished with value: 0.07390339554042 and parameters: {'learning_rate': 0.02, 'max_depth': 7, 'num_leaves': 591, 'min_child_samples': 6196, 'min_split_gain': 0.039079671568228794, 'feature_fraction': 0.32340279606636546, 'bagging_fraction': 0.6967983561008608, 'bagging_freq': 1, 'lambda_l1': 1.574189004745663, 'lambda_l2': 0.040428727350273294}. Best is trial 0 with value: 0.060481305339152816.\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[100]\ttrain's rmse: 0.247144\teval's rmse: 0.246425\n",
      "[200]\ttrain's rmse: 0.147253\teval's rmse: 0.146367\n",
      "[300]\ttrain's rmse: 0.125751\teval's rmse: 0.124898\n",
      "[400]\ttrain's rmse: 0.113277\teval's rmse: 0.112457\n",
      "[500]\ttrain's rmse: 0.102393\teval's rmse: 0.101625\n",
      "[600]\ttrain's rmse: 0.0973569\teval's rmse: 0.0966285\n",
      "[700]\ttrain's rmse: 0.0920247\teval's rmse: 0.0913365\n",
      "[800]\ttrain's rmse: 0.0891083\teval's rmse: 0.0884466\n",
      "[900]\ttrain's rmse: 0.0868937\teval's rmse: 0.0862589\n",
      "[1000]\ttrain's rmse: 0.0846356\teval's rmse: 0.0840264\n",
      "[1100]\ttrain's rmse: 0.08376\teval's rmse: 0.0831541\n",
      "[1200]\ttrain's rmse: 0.0831214\teval's rmse: 0.0825247\n",
      "[1300]\ttrain's rmse: 0.0824987\teval's rmse: 0.0819082\n",
      "[1400]\ttrain's rmse: 0.0819667\teval's rmse: 0.081382\n",
      "[1500]\ttrain's rmse: 0.0815509\teval's rmse: 0.0809711\n",
      "[1600]\ttrain's rmse: 0.0812752\teval's rmse: 0.0806969\n",
      "[1700]\ttrain's rmse: 0.0810597\teval's rmse: 0.0804824\n",
      "[1800]\ttrain's rmse: 0.0808411\teval's rmse: 0.0802649\n",
      "[1900]\ttrain's rmse: 0.0806536\teval's rmse: 0.0800794\n",
      "[2000]\ttrain's rmse: 0.080535\teval's rmse: 0.0799644\n",
      "[2100]\ttrain's rmse: 0.0804447\teval's rmse: 0.0798717\n",
      "[2200]\ttrain's rmse: 0.0803852\teval's rmse: 0.0798129\n",
      "[2300]\ttrain's rmse: 0.0803439\teval's rmse: 0.079771\n",
      "[2400]\ttrain's rmse: 0.0803433\teval's rmse: 0.0797704\n",
      "[2500]\ttrain's rmse: 0.0803433\teval's rmse: 0.0797704\n",
      "[2600]\ttrain's rmse: 0.0803433\teval's rmse: 0.0797704\n",
      "[2700]\ttrain's rmse: 0.0803433\teval's rmse: 0.0797704\n",
      "[2800]\ttrain's rmse: 0.0803433\teval's rmse: 0.0797704\n",
      "Early stopping, best iteration is:\n",
      "[2301]\ttrain's rmse: 0.0803433\teval's rmse: 0.0797704\n",
      "Training time: 00:00:55\n",
      "Train rmse: 0.08034\n",
      "Valid rmse: 0.07977\n",
      "[I 2025-07-28 01:23:32,368] Trial 12 finished with value: 0.07977039409116014 and parameters: {'learning_rate': 0.02, 'max_depth': 9, 'num_leaves': 404, 'min_child_samples': 6910, 'min_split_gain': 0.9877700294007907, 'feature_fraction': 0.3318508666017414, 'bagging_fraction': 0.7045474901621303, 'bagging_freq': 3, 'lambda_l1': 0.0006690421166498799, 'lambda_l2': 0.014077923139972392}. Best is trial 0 with value: 0.060481305339152816.\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[100]\ttrain's rmse: 0.24586\teval's rmse: 0.245158\n",
      "[200]\ttrain's rmse: 0.145433\teval's rmse: 0.144543\n",
      "[300]\ttrain's rmse: 0.123803\teval's rmse: 0.122972\n",
      "[400]\ttrain's rmse: 0.111271\teval's rmse: 0.110497\n",
      "[500]\ttrain's rmse: 0.100064\teval's rmse: 0.09935\n",
      "[600]\ttrain's rmse: 0.0947185\teval's rmse: 0.0940529\n",
      "[700]\ttrain's rmse: 0.0891849\teval's rmse: 0.0885616\n",
      "[800]\ttrain's rmse: 0.085633\teval's rmse: 0.0850373\n",
      "[900]\ttrain's rmse: 0.0830362\teval's rmse: 0.0824687\n",
      "[1000]\ttrain's rmse: 0.0805255\teval's rmse: 0.0799902\n",
      "[1100]\ttrain's rmse: 0.0791135\teval's rmse: 0.078591\n",
      "[1200]\ttrain's rmse: 0.0780878\teval's rmse: 0.0775749\n",
      "[1300]\ttrain's rmse: 0.0770572\teval's rmse: 0.0765598\n",
      "[1400]\ttrain's rmse: 0.0764012\teval's rmse: 0.0759141\n",
      "[1500]\ttrain's rmse: 0.0758644\teval's rmse: 0.0753857\n",
      "[1600]\ttrain's rmse: 0.0754185\teval's rmse: 0.0749509\n",
      "[1700]\ttrain's rmse: 0.0750411\teval's rmse: 0.0745757\n",
      "[1800]\ttrain's rmse: 0.0747188\teval's rmse: 0.0742618\n",
      "[1900]\ttrain's rmse: 0.0744228\teval's rmse: 0.0739745\n",
      "[2000]\ttrain's rmse: 0.0741834\teval's rmse: 0.0737382\n",
      "[2100]\ttrain's rmse: 0.0739836\teval's rmse: 0.073546\n",
      "[2200]\ttrain's rmse: 0.0738416\teval's rmse: 0.07341\n",
      "[2300]\ttrain's rmse: 0.0736863\teval's rmse: 0.0732585\n",
      "[2400]\ttrain's rmse: 0.0735346\teval's rmse: 0.0731114\n",
      "[2500]\ttrain's rmse: 0.0733951\teval's rmse: 0.0729753\n",
      "[2600]\ttrain's rmse: 0.0732816\teval's rmse: 0.0728656\n",
      "[2700]\ttrain's rmse: 0.0731793\teval's rmse: 0.0727667\n",
      "[2800]\ttrain's rmse: 0.0730797\teval's rmse: 0.0726705\n",
      "[2900]\ttrain's rmse: 0.0730074\teval's rmse: 0.0725997\n",
      "[3000]\ttrain's rmse: 0.072917\teval's rmse: 0.0725125\n",
      "[3100]\ttrain's rmse: 0.0728395\teval's rmse: 0.0724387\n",
      "[3200]\ttrain's rmse: 0.0727724\teval's rmse: 0.0723741\n",
      "[3300]\ttrain's rmse: 0.0727125\teval's rmse: 0.0723167\n",
      "[3400]\ttrain's rmse: 0.0726444\teval's rmse: 0.0722517\n",
      "[3500]\ttrain's rmse: 0.0725635\teval's rmse: 0.0721738\n",
      "[3600]\ttrain's rmse: 0.0724839\teval's rmse: 0.0720954\n",
      "[3700]\ttrain's rmse: 0.0724173\teval's rmse: 0.0720309\n",
      "[3800]\ttrain's rmse: 0.0723591\teval's rmse: 0.0719774\n",
      "[3900]\ttrain's rmse: 0.0723098\teval's rmse: 0.0719309\n",
      "[4000]\ttrain's rmse: 0.0722521\teval's rmse: 0.0718764\n",
      "[4100]\ttrain's rmse: 0.0721991\teval's rmse: 0.0718259\n",
      "[4200]\ttrain's rmse: 0.0721509\teval's rmse: 0.0717808\n",
      "[4300]\ttrain's rmse: 0.0721025\teval's rmse: 0.0717384\n",
      "[4400]\ttrain's rmse: 0.0720537\teval's rmse: 0.0716917\n",
      "[4500]\ttrain's rmse: 0.0720107\teval's rmse: 0.071651\n",
      "[4600]\ttrain's rmse: 0.0719703\teval's rmse: 0.0716128\n",
      "[4700]\ttrain's rmse: 0.0719255\teval's rmse: 0.0715695\n",
      "[4800]\ttrain's rmse: 0.0718816\teval's rmse: 0.0715288\n",
      "[4900]\ttrain's rmse: 0.0718531\teval's rmse: 0.0715039\n",
      "[5000]\ttrain's rmse: 0.071814\teval's rmse: 0.0714672\n",
      "[5100]\ttrain's rmse: 0.071771\teval's rmse: 0.0714266\n",
      "[5200]\ttrain's rmse: 0.0717277\teval's rmse: 0.0713854\n",
      "[5300]\ttrain's rmse: 0.0716961\teval's rmse: 0.0713577\n",
      "[5400]\ttrain's rmse: 0.0716656\teval's rmse: 0.0713301\n",
      "[5500]\ttrain's rmse: 0.0716353\teval's rmse: 0.0713027\n",
      "[5600]\ttrain's rmse: 0.0716016\teval's rmse: 0.07127\n",
      "[5700]\ttrain's rmse: 0.07157\teval's rmse: 0.0712415\n",
      "[5800]\ttrain's rmse: 0.0715426\teval's rmse: 0.0712163\n",
      "[5900]\ttrain's rmse: 0.0715141\teval's rmse: 0.0711916\n",
      "[6000]\ttrain's rmse: 0.0714802\teval's rmse: 0.0711603\n",
      "[6100]\ttrain's rmse: 0.0714539\teval's rmse: 0.0711374\n",
      "[6200]\ttrain's rmse: 0.0714291\teval's rmse: 0.0711153\n",
      "[6300]\ttrain's rmse: 0.0713995\teval's rmse: 0.0710889\n",
      "[6400]\ttrain's rmse: 0.0713761\teval's rmse: 0.0710684\n",
      "[6500]\ttrain's rmse: 0.0713552\teval's rmse: 0.0710502\n",
      "[6600]\ttrain's rmse: 0.0713292\teval's rmse: 0.0710272\n",
      "[6700]\ttrain's rmse: 0.0713038\teval's rmse: 0.0710045\n",
      "[6800]\ttrain's rmse: 0.0712764\teval's rmse: 0.0709797\n",
      "[6900]\ttrain's rmse: 0.0712524\teval's rmse: 0.0709574\n",
      "[7000]\ttrain's rmse: 0.0712294\teval's rmse: 0.0709371\n",
      "[7100]\ttrain's rmse: 0.0712081\teval's rmse: 0.0709186\n",
      "[7200]\ttrain's rmse: 0.0711855\teval's rmse: 0.0708987\n",
      "[7300]\ttrain's rmse: 0.0711662\teval's rmse: 0.0708833\n",
      "[7400]\ttrain's rmse: 0.0711415\teval's rmse: 0.070862\n",
      "[7500]\ttrain's rmse: 0.0711175\teval's rmse: 0.070839\n",
      "[7600]\ttrain's rmse: 0.0710972\teval's rmse: 0.0708205\n",
      "[7700]\ttrain's rmse: 0.0710756\teval's rmse: 0.0708003\n",
      "[7800]\ttrain's rmse: 0.0710583\teval's rmse: 0.0707865\n",
      "[7900]\ttrain's rmse: 0.0710377\teval's rmse: 0.0707693\n",
      "[8000]\ttrain's rmse: 0.0710204\teval's rmse: 0.0707531\n",
      "[8100]\ttrain's rmse: 0.0710039\teval's rmse: 0.070739\n",
      "[8200]\ttrain's rmse: 0.0709832\teval's rmse: 0.0707197\n",
      "[W 2025-07-28 01:34:32,799] Trial 13 failed with parameters: {'learning_rate': 0.02, 'max_depth': 7, 'num_leaves': 458, 'min_child_samples': 5836, 'min_split_gain': 6.870101665590028e-05, 'feature_fraction': 0.3438216972802827, 'bagging_fraction': 0.7599085529881076, 'bagging_freq': 7, 'lambda_l1': 0.5141096648805742, 'lambda_l2': 0.00015777663630582464} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/hanse/miniconda3/envs/rapids-23.12/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 201, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/mnt/c/Users/hanse/kaggle/calorie/src/models/lgbm/lgbm_optuna_optimizer.py\", line 62, in objective\n",
      "    trainer.fit_one_fold(tr_df, fold=0)\n",
      "  File \"/mnt/c/Users/hanse/kaggle/calorie/src/models/lgbm/lgbm_cv_trainer.py\", line 314, in fit_one_fold\n",
      "    model = lgb.train(\n",
      "  File \"/home/hanse/miniconda3/envs/rapids-23.12/lib/python3.10/site-packages/lightgbm/engine.py\", line 322, in train\n",
      "    booster.update(fobj=fobj)\n",
      "  File \"/home/hanse/miniconda3/envs/rapids-23.12/lib/python3.10/site-packages/lightgbm/basic.py\", line 4155, in update\n",
      "    _LIB.LGBM_BoosterUpdateOneIter(\n",
      "KeyboardInterrupt\n",
      "[W 2025-07-28 01:34:32,830] Trial 13 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 25\u001b[0m\n\u001b[1;32m     17\u001b[0m objective \u001b[38;5;241m=\u001b[39m op\u001b[38;5;241m.\u001b[39mcreate_objective(\n\u001b[1;32m     18\u001b[0m     tr_df1,\n\u001b[1;32m     19\u001b[0m     early_stopping_rounds\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m,\n\u001b[1;32m     20\u001b[0m     n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m25\u001b[39m\n\u001b[1;32m     21\u001b[0m )\n\u001b[1;32m     23\u001b[0m random_sampler \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39msamplers\u001b[38;5;241m.\u001b[39mRandomSampler(seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m---> 25\u001b[0m study \u001b[38;5;241m=\u001b[39m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_optuna_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstudy_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlgbm_v1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43msampler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptuna\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msamplers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTPESampler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_startup_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43minitial_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m te\u001b[38;5;241m.\u001b[39msend_telegram_message(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLGBM Training complete!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/mnt/c/Users/hanse/kaggle/calorie/src/models/lgbm/lgbm_optuna_optimizer.py:108\u001b[0m, in \u001b[0;36mrun_optuna_search\u001b[0;34m(objective, n_trials, n_jobs, study_name, storage, initial_params, sampler)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m initial_params \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    106\u001b[0m     study\u001b[38;5;241m.\u001b[39menqueue_trial(initial_params)\n\u001b[0;32m--> 108\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m    113\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m study\n",
      "File \u001b[0;32m~/miniconda3/envs/rapids-23.12/lib/python3.10/site-packages/optuna/study/study.py:489\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21moptimize\u001b[39m(\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    389\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    396\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    397\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    398\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    399\u001b[0m \n\u001b[1;32m    400\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    487\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    488\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 489\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/rapids-23.12/lib/python3.10/site-packages/optuna/study/_optimize.py:64\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 64\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     77\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/rapids-23.12/lib/python3.10/site-packages/optuna/study/_optimize.py:161\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 161\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m~/miniconda3/envs/rapids-23.12/lib/python3.10/site-packages/optuna/study/_optimize.py:253\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    249\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[1;32m    250\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    252\u001b[0m ):\n\u001b[0;32m--> 253\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m~/miniconda3/envs/rapids-23.12/lib/python3.10/site-packages/optuna/study/_optimize.py:201\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 201\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    204\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "File \u001b[0;32m/mnt/c/Users/hanse/kaggle/calorie/src/models/lgbm/lgbm_optuna_optimizer.py:62\u001b[0m, in \u001b[0;36mcreate_objective.<locals>.objective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     54\u001b[0m params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_depth\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_depth\u001b[39m\u001b[38;5;124m\"\u001b[39m], min_required_depth)\n\u001b[1;32m     56\u001b[0m trainer \u001b[38;5;241m=\u001b[39m LGBMCVTrainer(\n\u001b[1;32m     57\u001b[0m     params\u001b[38;5;241m=\u001b[39mparams,\n\u001b[1;32m     58\u001b[0m     n_splits\u001b[38;5;241m=\u001b[39mn_splits,\n\u001b[1;32m     59\u001b[0m     early_stopping_rounds\u001b[38;5;241m=\u001b[39mearly_stopping_rounds\n\u001b[1;32m     60\u001b[0m )\n\u001b[0;32m---> 62\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_one_fold\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mfold_scores[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/mnt/c/Users/hanse/kaggle/calorie/src/models/lgbm/lgbm_cv_trainer.py:314\u001b[0m, in \u001b[0;36mLGBMCVTrainer.fit_one_fold\u001b[0;34m(self, tr_df, fold)\u001b[0m\n\u001b[1;32m    310\u001b[0m dvalid \u001b[38;5;241m=\u001b[39m lgb\u001b[38;5;241m.\u001b[39mDataset(X_val, label\u001b[38;5;241m=\u001b[39my_val, reference\u001b[38;5;241m=\u001b[39mdtrain)\n\u001b[1;32m    312\u001b[0m evals_result \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m--> 314\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mlgb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_boost_round\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalid_sets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdvalid\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalid_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meval\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlgb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mearly_stopping\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlgb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecord_evaluation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevals_result\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlgb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_evaluation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mperiod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    327\u001b[0m end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    328\u001b[0m print_duration(start, end)\n",
      "File \u001b[0;32m~/miniconda3/envs/rapids-23.12/lib/python3.10/site-packages/lightgbm/engine.py:322\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, feval, init_model, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cb \u001b[38;5;129;01min\u001b[39;00m callbacks_before_iter:\n\u001b[1;32m    311\u001b[0m     cb(\n\u001b[1;32m    312\u001b[0m         callback\u001b[38;5;241m.\u001b[39mCallbackEnv(\n\u001b[1;32m    313\u001b[0m             model\u001b[38;5;241m=\u001b[39mbooster,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    319\u001b[0m         )\n\u001b[1;32m    320\u001b[0m     )\n\u001b[0;32m--> 322\u001b[0m \u001b[43mbooster\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    324\u001b[0m evaluation_result_list: List[_LGBM_BoosterEvalMethodResultType] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    325\u001b[0m \u001b[38;5;66;03m# check evaluation result.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/rapids-23.12/lib/python3.10/site-packages/lightgbm/basic.py:4155\u001b[0m, in \u001b[0;36mBooster.update\u001b[0;34m(self, train_set, fobj)\u001b[0m\n\u001b[1;32m   4152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__set_objective_to_none:\n\u001b[1;32m   4153\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LightGBMError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot update due to null objective function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   4154\u001b[0m _safe_call(\n\u001b[0;32m-> 4155\u001b[0m     \u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLGBM_BoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4156\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4157\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbyref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mis_finished\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4158\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4159\u001b[0m )\n\u001b[1;32m   4160\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__is_predicted_cur_iter \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__num_dataset)]\n\u001b[1;32m   4161\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m is_finished\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# tuning\n",
    "params = {\n",
    "    \"max_depth\": 5,\n",
    "    \"num_leaves\": 523,\n",
    "    \"min_child_samples\": 5056,\n",
    "    \"min_split_gain\": 9.275294835907687e-05,\n",
    "    \"feature_fraction\": 0.43593953046851464,\n",
    "    \"bagging_fraction\": 0.8924361138693251,\n",
    "    \"bagging_freq\": 10,\n",
    "    \"lambda_l1\": 1.6934155410667961,\n",
    "    \"lambda_l2\": 0.6637926838138378\n",
    "}\n",
    "\n",
    "objective = op.create_objective(\n",
    "    tr_df1,\n",
    "    early_stopping_rounds=500,\n",
    "    n_jobs=25\n",
    ")\n",
    "\n",
    "study = op.run_optuna_search(\n",
    "    objective,\n",
    "    n_trials=10,\n",
    "    n_jobs=1,\n",
    "    study_name=\"lgbm_v1\",\n",
    "    storage=url,\n",
    "    sampler=optuna.samplers.TPESampler(\n",
    "        n_startup_trials=20, seed=42),\n",
    "    initial_params=params\n",
    ")\n",
    "te.send_telegram_message(\"LGBM Training complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids-23.12",
   "language": "python",
   "name": "rapids-23.12"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
