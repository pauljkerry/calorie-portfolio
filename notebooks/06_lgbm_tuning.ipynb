{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9eb2fb4-1988-41de-88e3-2b07048a0062",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "import optuna\n",
    "import importlib\n",
    "\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "\n",
    "import src.models.lgbm.lgbm_cv_trainer as cv\n",
    "import src.models.lgbm.lgbm_optuna_optimizer as op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1443cab-6c7f-48e7-950e-2317ccc99379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# データの読み込み\n",
    "env_path = Path.cwd().parent / \".env\"\n",
    "load_dotenv(dotenv_path=env_path)\n",
    "url = os.environ.get(\"OPTUNA_STORAGE_URL\")\n",
    "\n",
    "tr_df1 = pd.read_csv(\"../artifacts/features/tr_df1.csv\")\n",
    "test_df1 = pd.read_csv(\"../artifacts/features/test_df1.csv\")\n",
    "\n",
    "cat_cols = [\"Sex\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac49c7d5-02bf-4f2f-b9a0-79a294a584e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-17 04:18:12,816] Using an existing study with name 'lgb_v1.1' instead of creating a new one.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a458f8e595714b5b9ec0c2447a16bb25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 500 rounds\n",
      "[100]\ttrain's rmse: 0.108151\teval's rmse: 0.106974\n",
      "[200]\ttrain's rmse: 0.0948969\teval's rmse: 0.0938244\n",
      "[300]\ttrain's rmse: 0.0909264\teval's rmse: 0.0899019\n",
      "[400]\ttrain's rmse: 0.0892709\teval's rmse: 0.088285\n",
      "[500]\ttrain's rmse: 0.0885346\teval's rmse: 0.0875684\n",
      "[600]\ttrain's rmse: 0.0883313\teval's rmse: 0.0873808\n",
      "[700]\ttrain's rmse: 0.0881251\teval's rmse: 0.0871906\n",
      "[800]\ttrain's rmse: 0.0880492\teval's rmse: 0.0871209\n",
      "[900]\ttrain's rmse: 0.087992\teval's rmse: 0.087069\n",
      "[1000]\ttrain's rmse: 0.0879148\teval's rmse: 0.0870015\n",
      "[1100]\ttrain's rmse: 0.0878489\teval's rmse: 0.0869393\n",
      "[1200]\ttrain's rmse: 0.0878252\teval's rmse: 0.0869172\n",
      "[1300]\ttrain's rmse: 0.0878134\teval's rmse: 0.0869062\n",
      "[1400]\ttrain's rmse: 0.0877965\teval's rmse: 0.0868924\n",
      "[1500]\ttrain's rmse: 0.0877821\teval's rmse: 0.0868795\n",
      "[1600]\ttrain's rmse: 0.0877718\teval's rmse: 0.0868699\n",
      "[1700]\ttrain's rmse: 0.0877646\teval's rmse: 0.0868649\n",
      "[1800]\ttrain's rmse: 0.0877559\teval's rmse: 0.0868574\n",
      "[1900]\ttrain's rmse: 0.0877482\teval's rmse: 0.0868516\n",
      "[2000]\ttrain's rmse: 0.0877434\teval's rmse: 0.0868486\n",
      "[2100]\ttrain's rmse: 0.08774\teval's rmse: 0.0868459\n",
      "[2200]\ttrain's rmse: 0.0877388\teval's rmse: 0.0868445\n",
      "[2300]\ttrain's rmse: 0.087729\teval's rmse: 0.0868362\n",
      "[2400]\ttrain's rmse: 0.0877257\teval's rmse: 0.0868335\n",
      "[2500]\ttrain's rmse: 0.0877206\teval's rmse: 0.0868308\n",
      "[2600]\ttrain's rmse: 0.0877176\teval's rmse: 0.0868285\n",
      "[2700]\ttrain's rmse: 0.0877172\teval's rmse: 0.0868285\n",
      "[2800]\ttrain's rmse: 0.087665\teval's rmse: 0.0867763\n",
      "[2900]\ttrain's rmse: 0.0876615\teval's rmse: 0.0867727\n",
      "[3000]\ttrain's rmse: 0.0876606\teval's rmse: 0.0867717\n",
      "[3100]\ttrain's rmse: 0.0876544\teval's rmse: 0.0867666\n",
      "[3200]\ttrain's rmse: 0.0876511\teval's rmse: 0.0867642\n",
      "[3300]\ttrain's rmse: 0.0876434\teval's rmse: 0.086758\n",
      "[3400]\ttrain's rmse: 0.0876324\teval's rmse: 0.0867491\n",
      "[3500]\ttrain's rmse: 0.0876275\teval's rmse: 0.0867462\n",
      "[3600]\ttrain's rmse: 0.0876263\teval's rmse: 0.0867455\n",
      "[3700]\ttrain's rmse: 0.0876257\teval's rmse: 0.0867449\n",
      "[3800]\ttrain's rmse: 0.0876218\teval's rmse: 0.0867414\n",
      "[3900]\ttrain's rmse: 0.0876176\teval's rmse: 0.0867369\n",
      "[4000]\ttrain's rmse: 0.0876174\teval's rmse: 0.0867373\n",
      "[4100]\ttrain's rmse: 0.0876145\teval's rmse: 0.0867352\n",
      "[4200]\ttrain's rmse: 0.0876132\teval's rmse: 0.0867345\n",
      "[4300]\ttrain's rmse: 0.0876124\teval's rmse: 0.0867334\n",
      "[4400]\ttrain's rmse: 0.0876121\teval's rmse: 0.0867334\n",
      "[4500]\ttrain's rmse: 0.0876096\teval's rmse: 0.0867313\n",
      "[4600]\ttrain's rmse: 0.0876094\teval's rmse: 0.0867316\n",
      "[4700]\ttrain's rmse: 0.0876061\teval's rmse: 0.0867285\n",
      "[4800]\ttrain's rmse: 0.0876051\teval's rmse: 0.0867282\n",
      "[4900]\ttrain's rmse: 0.0875654\teval's rmse: 0.0866872\n",
      "[5000]\ttrain's rmse: 0.0875559\teval's rmse: 0.0866797\n",
      "[5100]\ttrain's rmse: 0.0875495\teval's rmse: 0.0866744\n",
      "[5200]\ttrain's rmse: 0.087544\teval's rmse: 0.08667\n",
      "[5300]\ttrain's rmse: 0.0875431\teval's rmse: 0.0866703\n",
      "[5400]\ttrain's rmse: 0.0875427\teval's rmse: 0.0866703\n",
      "[5500]\ttrain's rmse: 0.0875424\teval's rmse: 0.0866707\n",
      "[5600]\ttrain's rmse: 0.0875403\teval's rmse: 0.0866693\n",
      "[5700]\ttrain's rmse: 0.0875401\teval's rmse: 0.0866687\n",
      "[5800]\ttrain's rmse: 0.0875399\teval's rmse: 0.0866695\n",
      "[5900]\ttrain's rmse: 0.0875386\teval's rmse: 0.0866689\n",
      "[6000]\ttrain's rmse: 0.0875328\teval's rmse: 0.0866628\n",
      "[6100]\ttrain's rmse: 0.0875317\teval's rmse: 0.0866621\n",
      "[6200]\ttrain's rmse: 0.0875299\teval's rmse: 0.0866612\n",
      "[6300]\ttrain's rmse: 0.0875282\teval's rmse: 0.0866599\n",
      "[6400]\ttrain's rmse: 0.087528\teval's rmse: 0.0866591\n",
      "[6500]\ttrain's rmse: 0.0875268\teval's rmse: 0.086658\n",
      "[6600]\ttrain's rmse: 0.0875265\teval's rmse: 0.0866575\n",
      "[6700]\ttrain's rmse: 0.0875234\teval's rmse: 0.086655\n",
      "[6800]\ttrain's rmse: 0.0875225\teval's rmse: 0.0866547\n",
      "[6900]\ttrain's rmse: 0.08751\teval's rmse: 0.0866434\n",
      "[7000]\ttrain's rmse: 0.0875049\teval's rmse: 0.0866398\n",
      "[7100]\ttrain's rmse: 0.0875041\teval's rmse: 0.08664\n",
      "[7200]\ttrain's rmse: 0.0875033\teval's rmse: 0.0866395\n",
      "[7300]\ttrain's rmse: 0.0875027\teval's rmse: 0.0866397\n",
      "[7400]\ttrain's rmse: 0.087502\teval's rmse: 0.0866387\n",
      "[7500]\ttrain's rmse: 0.0875004\teval's rmse: 0.0866369\n",
      "[7600]\ttrain's rmse: 0.087499\teval's rmse: 0.0866359\n",
      "[7700]\ttrain's rmse: 0.0874957\teval's rmse: 0.0866328\n",
      "[7800]\ttrain's rmse: 0.0874952\teval's rmse: 0.0866331\n",
      "[7900]\ttrain's rmse: 0.0874896\teval's rmse: 0.0866285\n",
      "[8000]\ttrain's rmse: 0.0874884\teval's rmse: 0.086628\n",
      "[8100]\ttrain's rmse: 0.0874867\teval's rmse: 0.0866263\n",
      "[8200]\ttrain's rmse: 0.0874858\teval's rmse: 0.0866263\n",
      "[8300]\ttrain's rmse: 0.0874851\teval's rmse: 0.0866253\n",
      "[8400]\ttrain's rmse: 0.0874845\teval's rmse: 0.0866242\n",
      "[8500]\ttrain's rmse: 0.0874842\teval's rmse: 0.0866245\n",
      "[8600]\ttrain's rmse: 0.0874789\teval's rmse: 0.0866203\n",
      "[8700]\ttrain's rmse: 0.0874735\teval's rmse: 0.0866145\n",
      "[8800]\ttrain's rmse: 0.0874729\teval's rmse: 0.0866147\n",
      "[8900]\ttrain's rmse: 0.0874727\teval's rmse: 0.0866143\n",
      "[9000]\ttrain's rmse: 0.0874721\teval's rmse: 0.0866148\n",
      "[9100]\ttrain's rmse: 0.0874704\teval's rmse: 0.0866132\n",
      "[9200]\ttrain's rmse: 0.087466\teval's rmse: 0.0866093\n",
      "[9300]\ttrain's rmse: 0.0874659\teval's rmse: 0.0866099\n",
      "[9400]\ttrain's rmse: 0.0874656\teval's rmse: 0.08661\n",
      "[9500]\ttrain's rmse: 0.087465\teval's rmse: 0.086609\n",
      "[9600]\ttrain's rmse: 0.0874643\teval's rmse: 0.0866083\n",
      "[9700]\ttrain's rmse: 0.0874604\teval's rmse: 0.0866055\n",
      "[9800]\ttrain's rmse: 0.087458\teval's rmse: 0.0866043\n",
      "[9900]\ttrain's rmse: 0.0874558\teval's rmse: 0.0866026\n",
      "[10000]\ttrain's rmse: 0.0874554\teval's rmse: 0.0866025\n",
      "[10100]\ttrain's rmse: 0.0874519\teval's rmse: 0.0865995\n",
      "[10200]\ttrain's rmse: 0.0874518\teval's rmse: 0.0865996\n",
      "[10300]\ttrain's rmse: 0.0874511\teval's rmse: 0.0865989\n",
      "[10400]\ttrain's rmse: 0.0874494\teval's rmse: 0.0865982\n",
      "[10500]\ttrain's rmse: 0.087447\teval's rmse: 0.086596\n",
      "[10600]\ttrain's rmse: 0.0874465\teval's rmse: 0.0865954\n",
      "[10700]\ttrain's rmse: 0.0874465\teval's rmse: 0.0865955\n",
      "[10800]\ttrain's rmse: 0.0874446\teval's rmse: 0.0865936\n",
      "[10900]\ttrain's rmse: 0.0874444\teval's rmse: 0.0865935\n",
      "[11000]\ttrain's rmse: 0.087444\teval's rmse: 0.0865936\n",
      "[11100]\ttrain's rmse: 0.0874435\teval's rmse: 0.0865935\n",
      "[11200]\ttrain's rmse: 0.087441\teval's rmse: 0.0865907\n",
      "[11300]\ttrain's rmse: 0.0874408\teval's rmse: 0.0865908\n",
      "[11400]\ttrain's rmse: 0.0874396\teval's rmse: 0.0865902\n",
      "[11500]\ttrain's rmse: 0.0874391\teval's rmse: 0.0865904\n",
      "[11600]\ttrain's rmse: 0.0874383\teval's rmse: 0.0865893\n",
      "[11700]\ttrain's rmse: 0.0874382\teval's rmse: 0.0865897\n",
      "[11800]\ttrain's rmse: 0.0874126\teval's rmse: 0.0865637\n",
      "[11900]\ttrain's rmse: 0.0874124\teval's rmse: 0.0865638\n",
      "[12000]\ttrain's rmse: 0.0874117\teval's rmse: 0.0865637\n",
      "[12100]\ttrain's rmse: 0.0874116\teval's rmse: 0.0865631\n",
      "[12200]\ttrain's rmse: 0.0874116\teval's rmse: 0.0865635\n",
      "[12300]\ttrain's rmse: 0.0874115\teval's rmse: 0.0865631\n",
      "[12400]\ttrain's rmse: 0.0874109\teval's rmse: 0.0865626\n",
      "[12500]\ttrain's rmse: 0.0874091\teval's rmse: 0.0865619\n",
      "[12600]\ttrain's rmse: 0.0874066\teval's rmse: 0.086559\n",
      "[12700]\ttrain's rmse: 0.0874049\teval's rmse: 0.0865589\n",
      "[12800]\ttrain's rmse: 0.0874049\teval's rmse: 0.0865591\n",
      "[12900]\ttrain's rmse: 0.0874048\teval's rmse: 0.086559\n",
      "[13000]\ttrain's rmse: 0.0874046\teval's rmse: 0.0865594\n",
      "[13100]\ttrain's rmse: 0.0874003\teval's rmse: 0.0865557\n",
      "[13200]\ttrain's rmse: 0.0874\teval's rmse: 0.086556\n",
      "[13300]\ttrain's rmse: 0.0873987\teval's rmse: 0.0865546\n",
      "[13400]\ttrain's rmse: 0.0873961\teval's rmse: 0.0865524\n",
      "[13500]\ttrain's rmse: 0.0873945\teval's rmse: 0.086551\n",
      "[13600]\ttrain's rmse: 0.0873944\teval's rmse: 0.0865508\n",
      "[13700]\ttrain's rmse: 0.0873932\teval's rmse: 0.0865498\n",
      "[13800]\ttrain's rmse: 0.0873895\teval's rmse: 0.0865473\n",
      "[13900]\ttrain's rmse: 0.087389\teval's rmse: 0.0865471\n",
      "[14000]\ttrain's rmse: 0.0873856\teval's rmse: 0.086543\n",
      "[14100]\ttrain's rmse: 0.0873855\teval's rmse: 0.0865429\n",
      "[14200]\ttrain's rmse: 0.0873853\teval's rmse: 0.0865424\n",
      "[14300]\ttrain's rmse: 0.0873843\teval's rmse: 0.0865425\n",
      "[14400]\ttrain's rmse: 0.0873832\teval's rmse: 0.0865419\n",
      "[14500]\ttrain's rmse: 0.0873829\teval's rmse: 0.0865418\n",
      "[14600]\ttrain's rmse: 0.0873829\teval's rmse: 0.0865421\n",
      "[14700]\ttrain's rmse: 0.0873827\teval's rmse: 0.086542\n",
      "[14800]\ttrain's rmse: 0.0873825\teval's rmse: 0.0865425\n",
      "[14900]\ttrain's rmse: 0.0873824\teval's rmse: 0.0865422\n",
      "Early stopping, best iteration is:\n",
      "[14430]\ttrain's rmse: 0.0873832\teval's rmse: 0.0865416\n",
      "Training time: 00:00:23\n",
      "Train rmse: 0.08738\n",
      "Valid rmse: 0.08654\n",
      "[I 2025-07-17 04:18:36,474] Trial 56 finished with value: 0.08654155616631103 and parameters: {'max_depth': 12, 'num_leaves': 786, 'min_child_samples': 11248, 'min_split_gain': 0.039079671568228794, 'feature_fraction': 0.2812037280884873, 'bagging_fraction': 0.6967983561008608, 'bagging_freq': 1, 'lambda_l1': 1.574189004745663, 'lambda_l2': 0.040428727350273294}. Best is trial 28 with value: 0.060545674331965615.\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[100]\ttrain's rmse: 0.112248\teval's rmse: 0.11107\n",
      "[200]\ttrain's rmse: 0.100471\teval's rmse: 0.0993701\n",
      "[300]\ttrain's rmse: 0.0974141\teval's rmse: 0.0963502\n",
      "[400]\ttrain's rmse: 0.0960331\teval's rmse: 0.0949885\n",
      "[500]\ttrain's rmse: 0.0954969\teval's rmse: 0.0944556\n",
      "[600]\ttrain's rmse: 0.0953848\teval's rmse: 0.0943443\n",
      "[700]\ttrain's rmse: 0.095295\teval's rmse: 0.0942578\n",
      "[800]\ttrain's rmse: 0.095295\teval's rmse: 0.0942578\n",
      "[900]\ttrain's rmse: 0.095295\teval's rmse: 0.0942578\n",
      "[1000]\ttrain's rmse: 0.095295\teval's rmse: 0.0942578\n",
      "[1100]\ttrain's rmse: 0.095295\teval's rmse: 0.0942578\n",
      "Early stopping, best iteration is:\n",
      "[657]\ttrain's rmse: 0.095295\teval's rmse: 0.0942578\n",
      "Training time: 00:00:02\n",
      "Train rmse: 0.09529\n",
      "Valid rmse: 0.09426\n",
      "[I 2025-07-17 04:18:38,980] Trial 57 finished with value: 0.0942577744494265 and parameters: {'max_depth': 14, 'num_leaves': 506, 'min_child_samples': 14579, 'min_split_gain': 0.9877700294007907, 'feature_fraction': 0.29246782213565525, 'bagging_fraction': 0.7045474901621303, 'bagging_freq': 3, 'lambda_l1': 0.0006690421166498799, 'lambda_l2': 0.014077923139972392}. Best is trial 28 with value: 0.060545674331965615.\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[100]\ttrain's rmse: 0.104356\teval's rmse: 0.103452\n",
      "[200]\ttrain's rmse: 0.0913368\teval's rmse: 0.0905361\n",
      "[300]\ttrain's rmse: 0.0871823\teval's rmse: 0.0864166\n",
      "[400]\ttrain's rmse: 0.0854334\teval's rmse: 0.0847101\n",
      "[500]\ttrain's rmse: 0.0845171\teval's rmse: 0.0838208\n",
      "[600]\ttrain's rmse: 0.0839914\teval's rmse: 0.0833196\n",
      "[700]\ttrain's rmse: 0.0835598\teval's rmse: 0.0829076\n",
      "[800]\ttrain's rmse: 0.0832546\teval's rmse: 0.082631\n",
      "[900]\ttrain's rmse: 0.0830231\teval's rmse: 0.0824252\n",
      "[1000]\ttrain's rmse: 0.0827361\teval's rmse: 0.0821626\n",
      "[1100]\ttrain's rmse: 0.0825292\teval's rmse: 0.0819724\n",
      "[1200]\ttrain's rmse: 0.0823074\teval's rmse: 0.0817697\n",
      "[1300]\ttrain's rmse: 0.0821378\teval's rmse: 0.0816159\n",
      "[1400]\ttrain's rmse: 0.0819852\teval's rmse: 0.0814793\n",
      "[1500]\ttrain's rmse: 0.0818705\teval's rmse: 0.0813735\n",
      "[1600]\ttrain's rmse: 0.0817535\teval's rmse: 0.0812749\n",
      "[1700]\ttrain's rmse: 0.0816376\teval's rmse: 0.0811727\n",
      "[1800]\ttrain's rmse: 0.081515\teval's rmse: 0.0810608\n",
      "[1900]\ttrain's rmse: 0.0814026\teval's rmse: 0.0809641\n",
      "[2000]\ttrain's rmse: 0.0813174\teval's rmse: 0.0808878\n",
      "[2100]\ttrain's rmse: 0.0812294\teval's rmse: 0.0808104\n",
      "[2200]\ttrain's rmse: 0.0811621\teval's rmse: 0.0807513\n",
      "[2300]\ttrain's rmse: 0.0811083\teval's rmse: 0.0807033\n",
      "[2400]\ttrain's rmse: 0.0810457\teval's rmse: 0.0806479\n",
      "[2500]\ttrain's rmse: 0.0809823\teval's rmse: 0.0805994\n",
      "[2600]\ttrain's rmse: 0.0809315\teval's rmse: 0.0805515\n",
      "[2700]\ttrain's rmse: 0.0808857\teval's rmse: 0.0805118\n",
      "[2800]\ttrain's rmse: 0.0808389\teval's rmse: 0.0804718\n",
      "[2900]\ttrain's rmse: 0.0808046\teval's rmse: 0.0804432\n",
      "[3000]\ttrain's rmse: 0.0807626\teval's rmse: 0.0804085\n",
      "[3100]\ttrain's rmse: 0.080725\teval's rmse: 0.0803756\n",
      "[3200]\ttrain's rmse: 0.0806906\teval's rmse: 0.0803486\n",
      "[3300]\ttrain's rmse: 0.0806582\teval's rmse: 0.0803226\n",
      "[3400]\ttrain's rmse: 0.0806171\teval's rmse: 0.08029\n",
      "[3500]\ttrain's rmse: 0.0805813\teval's rmse: 0.0802638\n",
      "[3600]\ttrain's rmse: 0.0805426\teval's rmse: 0.0802261\n",
      "[3700]\ttrain's rmse: 0.0805151\teval's rmse: 0.0802029\n",
      "[3800]\ttrain's rmse: 0.0804891\teval's rmse: 0.0801859\n",
      "[3900]\ttrain's rmse: 0.0804622\teval's rmse: 0.0801639\n",
      "[4000]\ttrain's rmse: 0.0804383\teval's rmse: 0.080142\n",
      "[4100]\ttrain's rmse: 0.0804083\teval's rmse: 0.0801215\n",
      "[4200]\ttrain's rmse: 0.0803824\teval's rmse: 0.080099\n",
      "[4300]\ttrain's rmse: 0.0803604\teval's rmse: 0.0800823\n",
      "[4400]\ttrain's rmse: 0.0803396\teval's rmse: 0.0800706\n",
      "[4500]\ttrain's rmse: 0.0803174\teval's rmse: 0.0800522\n",
      "[4600]\ttrain's rmse: 0.0802996\teval's rmse: 0.0800436\n",
      "[4700]\ttrain's rmse: 0.080281\teval's rmse: 0.0800285\n",
      "[4800]\ttrain's rmse: 0.0802594\teval's rmse: 0.0800104\n",
      "[4900]\ttrain's rmse: 0.0802443\teval's rmse: 0.0799994\n",
      "[5000]\ttrain's rmse: 0.0802227\teval's rmse: 0.0799848\n",
      "[5100]\ttrain's rmse: 0.0801977\teval's rmse: 0.0799638\n",
      "[5200]\ttrain's rmse: 0.0801781\teval's rmse: 0.0799496\n",
      "[5300]\ttrain's rmse: 0.0801611\teval's rmse: 0.0799405\n",
      "[5400]\ttrain's rmse: 0.0801486\teval's rmse: 0.0799323\n",
      "[5500]\ttrain's rmse: 0.0801332\teval's rmse: 0.0799162\n",
      "[5600]\ttrain's rmse: 0.0801185\teval's rmse: 0.0799074\n",
      "[5700]\ttrain's rmse: 0.0801043\teval's rmse: 0.0798983\n",
      "[5800]\ttrain's rmse: 0.0800908\teval's rmse: 0.0798875\n",
      "[5900]\ttrain's rmse: 0.0800779\teval's rmse: 0.0798807\n",
      "[6000]\ttrain's rmse: 0.0800611\teval's rmse: 0.0798676\n",
      "[6100]\ttrain's rmse: 0.0800453\teval's rmse: 0.0798558\n",
      "[6200]\ttrain's rmse: 0.0800291\teval's rmse: 0.0798441\n",
      "[6300]\ttrain's rmse: 0.0800143\teval's rmse: 0.0798359\n",
      "[6400]\ttrain's rmse: 0.0800017\teval's rmse: 0.0798264\n",
      "[6500]\ttrain's rmse: 0.0799895\teval's rmse: 0.0798218\n",
      "[6600]\ttrain's rmse: 0.0799746\teval's rmse: 0.0798099\n",
      "[6700]\ttrain's rmse: 0.0799641\teval's rmse: 0.0798041\n",
      "[6800]\ttrain's rmse: 0.0799514\teval's rmse: 0.0797968\n",
      "[6900]\ttrain's rmse: 0.0799403\teval's rmse: 0.0797905\n",
      "[7000]\ttrain's rmse: 0.0799281\teval's rmse: 0.0797832\n",
      "[7100]\ttrain's rmse: 0.0799176\teval's rmse: 0.0797761\n",
      "[7200]\ttrain's rmse: 0.0799065\teval's rmse: 0.0797709\n",
      "[7300]\ttrain's rmse: 0.0798946\teval's rmse: 0.0797618\n",
      "[7400]\ttrain's rmse: 0.0798827\teval's rmse: 0.0797554\n",
      "[7500]\ttrain's rmse: 0.0798731\teval's rmse: 0.0797473\n",
      "[7600]\ttrain's rmse: 0.0798636\teval's rmse: 0.0797429\n",
      "[7700]\ttrain's rmse: 0.0798537\teval's rmse: 0.0797356\n",
      "[7800]\ttrain's rmse: 0.0798446\teval's rmse: 0.079732\n",
      "[7900]\ttrain's rmse: 0.0798317\teval's rmse: 0.0797229\n",
      "[8000]\ttrain's rmse: 0.0798216\teval's rmse: 0.0797148\n",
      "[8100]\ttrain's rmse: 0.0798123\teval's rmse: 0.0797083\n",
      "[8200]\ttrain's rmse: 0.0798027\teval's rmse: 0.0797029\n",
      "[8300]\ttrain's rmse: 0.0797943\teval's rmse: 0.0796973\n",
      "[8400]\ttrain's rmse: 0.0797862\teval's rmse: 0.0796909\n",
      "[8500]\ttrain's rmse: 0.0797783\teval's rmse: 0.0796855\n",
      "[8600]\ttrain's rmse: 0.0797704\teval's rmse: 0.0796806\n",
      "[8700]\ttrain's rmse: 0.0797621\teval's rmse: 0.0796754\n",
      "[8800]\ttrain's rmse: 0.0797555\teval's rmse: 0.0796733\n",
      "[8900]\ttrain's rmse: 0.0797496\teval's rmse: 0.0796729\n",
      "[9000]\ttrain's rmse: 0.0797416\teval's rmse: 0.079668\n",
      "[9100]\ttrain's rmse: 0.0797332\teval's rmse: 0.0796625\n",
      "[9200]\ttrain's rmse: 0.0797253\teval's rmse: 0.0796619\n",
      "[9300]\ttrain's rmse: 0.0797182\teval's rmse: 0.0796571\n",
      "[9400]\ttrain's rmse: 0.0797112\teval's rmse: 0.0796527\n",
      "[9500]\ttrain's rmse: 0.0797047\teval's rmse: 0.0796507\n",
      "[9600]\ttrain's rmse: 0.0796978\teval's rmse: 0.0796463\n",
      "[9700]\ttrain's rmse: 0.0796921\teval's rmse: 0.0796413\n",
      "[9800]\ttrain's rmse: 0.0796853\teval's rmse: 0.0796382\n",
      "[9900]\ttrain's rmse: 0.0796777\teval's rmse: 0.0796337\n",
      "[10000]\ttrain's rmse: 0.0796701\teval's rmse: 0.0796298\n",
      "[10100]\ttrain's rmse: 0.079664\teval's rmse: 0.0796249\n",
      "[10200]\ttrain's rmse: 0.0796581\teval's rmse: 0.0796227\n",
      "[10300]\ttrain's rmse: 0.0796502\teval's rmse: 0.0796194\n",
      "[10400]\ttrain's rmse: 0.0796419\teval's rmse: 0.0796129\n",
      "[10500]\ttrain's rmse: 0.0796362\teval's rmse: 0.0796117\n",
      "[10600]\ttrain's rmse: 0.0796311\teval's rmse: 0.0796064\n",
      "[10700]\ttrain's rmse: 0.079626\teval's rmse: 0.0796053\n",
      "[10800]\ttrain's rmse: 0.0796213\teval's rmse: 0.0796049\n",
      "[10900]\ttrain's rmse: 0.079617\teval's rmse: 0.0795985\n",
      "[11000]\ttrain's rmse: 0.0796099\teval's rmse: 0.0795937\n",
      "[11100]\ttrain's rmse: 0.0796049\teval's rmse: 0.0795947\n",
      "[11200]\ttrain's rmse: 0.0795971\teval's rmse: 0.0795917\n",
      "[11300]\ttrain's rmse: 0.0795915\teval's rmse: 0.0795908\n",
      "[11400]\ttrain's rmse: 0.0795849\teval's rmse: 0.0795852\n",
      "[11500]\ttrain's rmse: 0.0795807\teval's rmse: 0.0795843\n",
      "[11600]\ttrain's rmse: 0.079575\teval's rmse: 0.0795813\n",
      "[11700]\ttrain's rmse: 0.0795705\teval's rmse: 0.0795768\n",
      "[11800]\ttrain's rmse: 0.0795645\teval's rmse: 0.0795739\n",
      "[11900]\ttrain's rmse: 0.0795602\teval's rmse: 0.0795691\n",
      "[12000]\ttrain's rmse: 0.0795553\teval's rmse: 0.0795683\n",
      "[12100]\ttrain's rmse: 0.0795502\teval's rmse: 0.0795662\n",
      "[12200]\ttrain's rmse: 0.0795445\teval's rmse: 0.0795645\n",
      "[12300]\ttrain's rmse: 0.0795391\teval's rmse: 0.0795582\n",
      "[12400]\ttrain's rmse: 0.0795335\teval's rmse: 0.0795529\n",
      "[12500]\ttrain's rmse: 0.0795285\teval's rmse: 0.0795532\n",
      "[12600]\ttrain's rmse: 0.0795228\teval's rmse: 0.0795468\n",
      "[12700]\ttrain's rmse: 0.079518\teval's rmse: 0.0795459\n",
      "[12800]\ttrain's rmse: 0.0795132\teval's rmse: 0.0795435\n",
      "[12900]\ttrain's rmse: 0.0795079\teval's rmse: 0.0795438\n",
      "[13000]\ttrain's rmse: 0.0795039\teval's rmse: 0.0795426\n",
      "[13100]\ttrain's rmse: 0.0794999\teval's rmse: 0.0795391\n",
      "[13200]\ttrain's rmse: 0.0794957\teval's rmse: 0.0795387\n",
      "[13300]\ttrain's rmse: 0.0794904\teval's rmse: 0.0795358\n",
      "[13400]\ttrain's rmse: 0.0794869\teval's rmse: 0.0795331\n",
      "[13500]\ttrain's rmse: 0.0794818\teval's rmse: 0.079531\n",
      "[13600]\ttrain's rmse: 0.0794769\teval's rmse: 0.0795285\n",
      "[13700]\ttrain's rmse: 0.0794727\teval's rmse: 0.0795273\n",
      "[13800]\ttrain's rmse: 0.0794673\teval's rmse: 0.0795237\n",
      "[13900]\ttrain's rmse: 0.0794643\teval's rmse: 0.0795216\n",
      "[14000]\ttrain's rmse: 0.0794591\teval's rmse: 0.0795183\n",
      "[14100]\ttrain's rmse: 0.0794551\teval's rmse: 0.0795141\n",
      "[14200]\ttrain's rmse: 0.0794507\teval's rmse: 0.0795142\n",
      "[14300]\ttrain's rmse: 0.0794473\teval's rmse: 0.0795141\n",
      "[14400]\ttrain's rmse: 0.0794433\teval's rmse: 0.0795136\n",
      "[14500]\ttrain's rmse: 0.0794397\teval's rmse: 0.0795145\n",
      "[14600]\ttrain's rmse: 0.0794349\teval's rmse: 0.0795103\n",
      "[14700]\ttrain's rmse: 0.079431\teval's rmse: 0.0795077\n",
      "[14800]\ttrain's rmse: 0.0794278\teval's rmse: 0.0795081\n",
      "[14900]\ttrain's rmse: 0.0794238\teval's rmse: 0.0795085\n",
      "[15000]\ttrain's rmse: 0.0794202\teval's rmse: 0.0795077\n",
      "[15100]\ttrain's rmse: 0.0794167\teval's rmse: 0.0795096\n",
      "[15200]\ttrain's rmse: 0.0794135\teval's rmse: 0.0795118\n",
      "[15300]\ttrain's rmse: 0.07941\teval's rmse: 0.0795097\n",
      "Early stopping, best iteration is:\n",
      "[14810]\ttrain's rmse: 0.0794276\teval's rmse: 0.0795072\n",
      "Training time: 00:02:04\n",
      "Train rmse: 0.07943\n",
      "Valid rmse: 0.07951\n",
      "[I 2025-07-17 04:20:43,462] Trial 58 finished with value: 0.07950719570821173 and parameters: {'max_depth': 12, 'num_leaves': 587, 'min_child_samples': 9566, 'min_split_gain': 6.870101665590028e-05, 'feature_fraction': 0.3084289297070436, 'bagging_fraction': 0.7599085529881076, 'bagging_freq': 7, 'lambda_l1': 0.5141096648805742, 'lambda_l2': 0.00015777663630582464}. Best is trial 28 with value: 0.060545674331965615.\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[100]\ttrain's rmse: 0.0917478\teval's rmse: 0.0911712\n",
      "[200]\ttrain's rmse: 0.0785427\teval's rmse: 0.0780617\n",
      "[300]\ttrain's rmse: 0.0754421\teval's rmse: 0.0750061\n",
      "[400]\ttrain's rmse: 0.074416\teval's rmse: 0.073992\n",
      "[500]\ttrain's rmse: 0.0739965\teval's rmse: 0.07358\n",
      "[600]\ttrain's rmse: 0.0739537\teval's rmse: 0.0735371\n",
      "[700]\ttrain's rmse: 0.0739124\teval's rmse: 0.0734951\n",
      "[800]\ttrain's rmse: 0.0739001\teval's rmse: 0.0734821\n",
      "[900]\ttrain's rmse: 0.0738912\teval's rmse: 0.0734733\n",
      "[1000]\ttrain's rmse: 0.0738912\teval's rmse: 0.0734733\n",
      "[1100]\ttrain's rmse: 0.0738912\teval's rmse: 0.0734733\n",
      "[1200]\ttrain's rmse: 0.0738912\teval's rmse: 0.0734733\n",
      "[1300]\ttrain's rmse: 0.0738912\teval's rmse: 0.0734733\n",
      "Early stopping, best iteration is:\n",
      "[837]\ttrain's rmse: 0.0738912\teval's rmse: 0.0734733\n",
      "Training time: 00:00:03\n",
      "Train rmse: 0.07389\n",
      "Valid rmse: 0.07347\n",
      "[I 2025-07-17 04:20:47,292] Trial 59 finished with value: 0.0734733131780729 and parameters: {'max_depth': 13, 'num_leaves': 678, 'min_child_samples': 1650, 'min_split_gain': 0.0441844152119972, 'feature_fraction': 0.2841048247374583, 'bagging_fraction': 0.6695154778955839, 'bagging_freq': 15, 'lambda_l1': 6.220025976819156, 'lambda_l2': 0.7085721663941598}. Best is trial 28 with value: 0.060545674331965615.\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[100]\ttrain's rmse: 0.105034\teval's rmse: 0.104128\n",
      "[200]\ttrain's rmse: 0.0918107\teval's rmse: 0.0910241\n",
      "[300]\ttrain's rmse: 0.0876963\teval's rmse: 0.0869629\n",
      "[400]\ttrain's rmse: 0.0859241\teval's rmse: 0.0852298\n",
      "[500]\ttrain's rmse: 0.0850321\teval's rmse: 0.0843763\n",
      "[600]\ttrain's rmse: 0.0846499\teval's rmse: 0.0840073\n",
      "[700]\ttrain's rmse: 0.084285\teval's rmse: 0.083665\n",
      "[800]\ttrain's rmse: 0.0840254\teval's rmse: 0.083424\n",
      "[900]\ttrain's rmse: 0.0838308\teval's rmse: 0.0832444\n",
      "[1000]\ttrain's rmse: 0.0835953\teval's rmse: 0.083033\n",
      "[1100]\ttrain's rmse: 0.0834498\teval's rmse: 0.0829056\n",
      "[1200]\ttrain's rmse: 0.0832667\teval's rmse: 0.0827346\n",
      "[1300]\ttrain's rmse: 0.0831647\teval's rmse: 0.082641\n",
      "[1400]\ttrain's rmse: 0.0830849\teval's rmse: 0.0825685\n",
      "[1500]\ttrain's rmse: 0.082994\teval's rmse: 0.0824843\n",
      "[1600]\ttrain's rmse: 0.0829174\teval's rmse: 0.0824172\n",
      "[1700]\ttrain's rmse: 0.0828265\teval's rmse: 0.0823355\n",
      "[1800]\ttrain's rmse: 0.0827384\teval's rmse: 0.0822542\n",
      "[1900]\ttrain's rmse: 0.0826402\teval's rmse: 0.0821639\n",
      "[2000]\ttrain's rmse: 0.0825874\teval's rmse: 0.0821129\n",
      "[2100]\ttrain's rmse: 0.0825309\teval's rmse: 0.0820627\n",
      "[2200]\ttrain's rmse: 0.0824874\teval's rmse: 0.0820219\n",
      "[2300]\ttrain's rmse: 0.0824438\teval's rmse: 0.0819817\n",
      "[2400]\ttrain's rmse: 0.0824086\teval's rmse: 0.0819521\n",
      "[2500]\ttrain's rmse: 0.0823573\teval's rmse: 0.0819055\n",
      "[2600]\ttrain's rmse: 0.0823037\teval's rmse: 0.0818594\n",
      "[2700]\ttrain's rmse: 0.0822832\teval's rmse: 0.0818394\n",
      "[2800]\ttrain's rmse: 0.082256\teval's rmse: 0.081814\n",
      "[2900]\ttrain's rmse: 0.0822297\teval's rmse: 0.0817887\n",
      "[3000]\ttrain's rmse: 0.0821936\teval's rmse: 0.0817548\n",
      "[3100]\ttrain's rmse: 0.0821625\teval's rmse: 0.0817236\n",
      "[3200]\ttrain's rmse: 0.0821408\teval's rmse: 0.0817022\n",
      "[3300]\ttrain's rmse: 0.082105\teval's rmse: 0.0816717\n",
      "[3400]\ttrain's rmse: 0.0820785\teval's rmse: 0.0816536\n",
      "[3500]\ttrain's rmse: 0.0820409\teval's rmse: 0.0816189\n",
      "[3600]\ttrain's rmse: 0.0820171\teval's rmse: 0.0815952\n",
      "[3700]\ttrain's rmse: 0.0819984\teval's rmse: 0.0815782\n",
      "[3800]\ttrain's rmse: 0.0819686\teval's rmse: 0.0815548\n",
      "[3900]\ttrain's rmse: 0.0819463\teval's rmse: 0.0815372\n",
      "[4000]\ttrain's rmse: 0.0819311\teval's rmse: 0.0815221\n",
      "[4100]\ttrain's rmse: 0.0819109\teval's rmse: 0.0815016\n",
      "[4200]\ttrain's rmse: 0.0818919\teval's rmse: 0.0814854\n",
      "[4300]\ttrain's rmse: 0.0818684\teval's rmse: 0.0814671\n",
      "[4400]\ttrain's rmse: 0.0818489\teval's rmse: 0.0814509\n",
      "[4500]\ttrain's rmse: 0.0818316\teval's rmse: 0.0814337\n",
      "[4600]\ttrain's rmse: 0.0818164\teval's rmse: 0.0814211\n",
      "[4700]\ttrain's rmse: 0.0818004\teval's rmse: 0.0814065\n",
      "[4800]\ttrain's rmse: 0.0817915\teval's rmse: 0.0813997\n",
      "[4900]\ttrain's rmse: 0.0817801\teval's rmse: 0.0813873\n",
      "[5000]\ttrain's rmse: 0.081758\teval's rmse: 0.0813683\n",
      "[5100]\ttrain's rmse: 0.0817374\teval's rmse: 0.0813497\n",
      "[5200]\ttrain's rmse: 0.0817204\teval's rmse: 0.0813344\n",
      "[5300]\ttrain's rmse: 0.0817076\teval's rmse: 0.0813223\n",
      "[5400]\ttrain's rmse: 0.0816983\teval's rmse: 0.0813137\n",
      "[5500]\ttrain's rmse: 0.0816854\teval's rmse: 0.081301\n",
      "[5600]\ttrain's rmse: 0.0816734\teval's rmse: 0.0812912\n",
      "[5700]\ttrain's rmse: 0.0816641\teval's rmse: 0.0812837\n",
      "[5800]\ttrain's rmse: 0.081653\teval's rmse: 0.0812746\n",
      "[5900]\ttrain's rmse: 0.0816381\teval's rmse: 0.0812618\n",
      "[6000]\ttrain's rmse: 0.0816254\teval's rmse: 0.0812498\n",
      "[6100]\ttrain's rmse: 0.0816121\teval's rmse: 0.0812397\n",
      "[6200]\ttrain's rmse: 0.0816025\teval's rmse: 0.0812327\n",
      "[6300]\ttrain's rmse: 0.0815854\teval's rmse: 0.0812142\n",
      "[6400]\ttrain's rmse: 0.0815738\teval's rmse: 0.0812035\n",
      "[6500]\ttrain's rmse: 0.0815633\teval's rmse: 0.0811949\n",
      "[6600]\ttrain's rmse: 0.0815496\teval's rmse: 0.0811817\n",
      "[6700]\ttrain's rmse: 0.0815421\teval's rmse: 0.0811759\n",
      "[6800]\ttrain's rmse: 0.081529\teval's rmse: 0.0811663\n",
      "[6900]\ttrain's rmse: 0.0815176\teval's rmse: 0.081154\n",
      "[7000]\ttrain's rmse: 0.0815102\teval's rmse: 0.081148\n",
      "[7100]\ttrain's rmse: 0.0815009\teval's rmse: 0.0811409\n",
      "[7200]\ttrain's rmse: 0.0814934\teval's rmse: 0.081134\n",
      "[7300]\ttrain's rmse: 0.0814828\teval's rmse: 0.0811252\n",
      "[7400]\ttrain's rmse: 0.0814746\teval's rmse: 0.081119\n",
      "[7500]\ttrain's rmse: 0.0814652\teval's rmse: 0.081108\n",
      "[7600]\ttrain's rmse: 0.0814587\teval's rmse: 0.081102\n",
      "[7700]\ttrain's rmse: 0.0814448\teval's rmse: 0.0810889\n",
      "[7800]\ttrain's rmse: 0.0814393\teval's rmse: 0.0810855\n",
      "[7900]\ttrain's rmse: 0.0814283\teval's rmse: 0.0810736\n",
      "[8000]\ttrain's rmse: 0.081419\teval's rmse: 0.0810662\n",
      "[8100]\ttrain's rmse: 0.081407\teval's rmse: 0.081055\n",
      "[8200]\ttrain's rmse: 0.0813985\teval's rmse: 0.0810469\n",
      "[8300]\ttrain's rmse: 0.0813895\teval's rmse: 0.08104\n",
      "[8400]\ttrain's rmse: 0.0813834\teval's rmse: 0.0810349\n",
      "[8500]\ttrain's rmse: 0.0813754\teval's rmse: 0.0810286\n",
      "[8600]\ttrain's rmse: 0.0813672\teval's rmse: 0.0810206\n",
      "[8700]\ttrain's rmse: 0.0813601\teval's rmse: 0.0810136\n",
      "[8800]\ttrain's rmse: 0.0813532\teval's rmse: 0.0810075\n",
      "[8900]\ttrain's rmse: 0.0813457\teval's rmse: 0.0809986\n",
      "[9000]\ttrain's rmse: 0.0813382\teval's rmse: 0.0809947\n",
      "[9100]\ttrain's rmse: 0.0813338\teval's rmse: 0.080991\n",
      "[9200]\ttrain's rmse: 0.0813263\teval's rmse: 0.0809849\n",
      "[9300]\ttrain's rmse: 0.081321\teval's rmse: 0.0809814\n",
      "[9400]\ttrain's rmse: 0.0813149\teval's rmse: 0.0809747\n",
      "[9500]\ttrain's rmse: 0.081311\teval's rmse: 0.0809721\n",
      "[9600]\ttrain's rmse: 0.0813081\teval's rmse: 0.0809685\n",
      "[9700]\ttrain's rmse: 0.0813001\teval's rmse: 0.0809627\n",
      "[9800]\ttrain's rmse: 0.0812968\teval's rmse: 0.0809588\n",
      "[9900]\ttrain's rmse: 0.081293\teval's rmse: 0.0809559\n",
      "[10000]\ttrain's rmse: 0.0812895\teval's rmse: 0.0809524\n",
      "[10100]\ttrain's rmse: 0.0812862\teval's rmse: 0.08095\n",
      "[10200]\ttrain's rmse: 0.0812821\teval's rmse: 0.0809478\n",
      "[10300]\ttrain's rmse: 0.0812768\teval's rmse: 0.080944\n",
      "[10400]\ttrain's rmse: 0.0812728\teval's rmse: 0.0809398\n",
      "[10500]\ttrain's rmse: 0.0812656\teval's rmse: 0.0809335\n",
      "[10600]\ttrain's rmse: 0.0812617\teval's rmse: 0.0809291\n",
      "[10700]\ttrain's rmse: 0.081258\teval's rmse: 0.0809248\n",
      "[10800]\ttrain's rmse: 0.0812514\teval's rmse: 0.0809187\n",
      "[10900]\ttrain's rmse: 0.0812466\teval's rmse: 0.0809155\n",
      "[11000]\ttrain's rmse: 0.0812428\teval's rmse: 0.0809131\n",
      "[11100]\ttrain's rmse: 0.0812388\teval's rmse: 0.080908\n",
      "[11200]\ttrain's rmse: 0.0812352\teval's rmse: 0.0809045\n",
      "[11300]\ttrain's rmse: 0.0812302\teval's rmse: 0.0809011\n",
      "[11400]\ttrain's rmse: 0.081227\teval's rmse: 0.0808975\n",
      "[11500]\ttrain's rmse: 0.0812225\teval's rmse: 0.0808931\n",
      "[11600]\ttrain's rmse: 0.0812174\teval's rmse: 0.0808881\n",
      "[11700]\ttrain's rmse: 0.0812112\teval's rmse: 0.0808828\n",
      "[11800]\ttrain's rmse: 0.0812064\teval's rmse: 0.0808784\n",
      "[11900]\ttrain's rmse: 0.0812027\teval's rmse: 0.0808739\n",
      "[12000]\ttrain's rmse: 0.0812004\teval's rmse: 0.0808723\n",
      "[12100]\ttrain's rmse: 0.0811968\teval's rmse: 0.0808693\n",
      "[12200]\ttrain's rmse: 0.0811927\teval's rmse: 0.0808635\n",
      "[12300]\ttrain's rmse: 0.0811877\teval's rmse: 0.0808604\n",
      "[12400]\ttrain's rmse: 0.0811815\teval's rmse: 0.080854\n",
      "[12500]\ttrain's rmse: 0.0811763\teval's rmse: 0.0808511\n",
      "[12600]\ttrain's rmse: 0.0811702\teval's rmse: 0.0808427\n",
      "[12700]\ttrain's rmse: 0.0811673\teval's rmse: 0.0808408\n",
      "[12800]\ttrain's rmse: 0.0811631\teval's rmse: 0.0808367\n",
      "[12900]\ttrain's rmse: 0.0811603\teval's rmse: 0.0808346\n",
      "[13000]\ttrain's rmse: 0.0811551\teval's rmse: 0.0808306\n",
      "[13100]\ttrain's rmse: 0.0811509\teval's rmse: 0.080825\n",
      "[13200]\ttrain's rmse: 0.0811479\teval's rmse: 0.0808234\n",
      "[W 2025-07-17 04:21:23,161] Trial 60 failed with parameters: {'max_depth': 11, 'num_leaves': 529, 'min_child_samples': 10579, 'min_split_gain': 0.004374364439939076, 'feature_fraction': 0.27440764696895575, 'bagging_fraction': 0.7985530730333811, 'bagging_freq': 1, 'lambda_l1': 2.857080075040721, 'lambda_l2': 0.0003570095960030948} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\hanse\\anaconda3\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 201, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"C:\\Users\\hanse\\kaggle\\calorie\\src\\models\\lgbm\\lgbm_optuna_optimizer.py\", line 65, in objective\n",
      "    trainer.fit_one_fold(tr_df, fold=0)\n",
      "    ~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\hanse\\kaggle\\calorie\\src\\models\\lgbm\\lgbm_cv_trainer.py\", line 338, in fit_one_fold\n",
      "    model = lgb.train(\n",
      "        self.params,\n",
      "    ...<8 lines>...\n",
      "        ]\n",
      "    )\n",
      "  File \"C:\\Users\\hanse\\anaconda3\\Lib\\site-packages\\lightgbm\\engine.py\", line 328, in train\n",
      "    evaluation_result_list.extend(booster.eval_train(feval))\n",
      "                                  ~~~~~~~~~~~~~~~~~~^^^^^^^\n",
      "  File \"C:\\Users\\hanse\\anaconda3\\Lib\\site-packages\\lightgbm\\basic.py\", line 4408, in eval_train\n",
      "    return self.__inner_eval(self._train_data_name, 0, feval)\n",
      "           ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\hanse\\anaconda3\\Lib\\site-packages\\lightgbm\\basic.py\", line 5185, in __inner_eval\n",
      "    _LIB.LGBM_BoosterGetEval(\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "        self._handle,\n",
      "        ^^^^^^^^^^^^^\n",
      "    ...<2 lines>...\n",
      "        result.ctypes.data_as(ctypes.POINTER(ctypes.c_double)),\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "KeyboardInterrupt\n",
      "[W 2025-07-17 04:21:23,168] Trial 60 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 13\u001b[0m\n\u001b[0;32m      4\u001b[0m objective \u001b[38;5;241m=\u001b[39m op\u001b[38;5;241m.\u001b[39mcreate_objective(\n\u001b[0;32m      5\u001b[0m     tr_df1,\n\u001b[0;32m      6\u001b[0m     early_stopping_rounds\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m,\n\u001b[0;32m      7\u001b[0m     cat_cols\u001b[38;5;241m=\u001b[39mcat_cols,\n\u001b[0;32m      8\u001b[0m     n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m25\u001b[39m\n\u001b[0;32m      9\u001b[0m )\n\u001b[0;32m     11\u001b[0m random_sampler \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39msamplers\u001b[38;5;241m.\u001b[39mRandomSampler(seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m---> 13\u001b[0m study \u001b[38;5;241m=\u001b[39m op\u001b[38;5;241m.\u001b[39mrun_optuna_search(\n\u001b[0;32m     14\u001b[0m     objective,\n\u001b[0;32m     15\u001b[0m     n_trials\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[0;32m     16\u001b[0m     n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m     17\u001b[0m     study_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlgb_v1.1\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     18\u001b[0m     storage\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m     19\u001b[0m     sampler\u001b[38;5;241m=\u001b[39mrandom_sampler\n\u001b[0;32m     20\u001b[0m )\n",
      "File \u001b[1;32m~\\kaggle\\calorie\\src\\models\\lgbm\\lgbm_optuna_optimizer.py:114\u001b[0m, in \u001b[0;36mrun_optuna_search\u001b[1;34m(objective, n_trials, n_jobs, study_name, storage, initial_params, sampler)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m initial_params \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    112\u001b[0m     study\u001b[38;5;241m.\u001b[39menqueue_trial(initial_params)\n\u001b[1;32m--> 114\u001b[0m study\u001b[38;5;241m.\u001b[39moptimize(\n\u001b[0;32m    115\u001b[0m     objective,\n\u001b[0;32m    116\u001b[0m     n_trials\u001b[38;5;241m=\u001b[39mn_trials,\n\u001b[0;32m    117\u001b[0m     n_jobs\u001b[38;5;241m=\u001b[39mn_jobs,\n\u001b[0;32m    118\u001b[0m     show_progress_bar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    119\u001b[0m )\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m study\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\optuna\\study\\study.py:489\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21moptimize\u001b[39m(\n\u001b[0;32m    388\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    389\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    396\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    397\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    398\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    399\u001b[0m \n\u001b[0;32m    400\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    487\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    488\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 489\u001b[0m     _optimize(\n\u001b[0;32m    490\u001b[0m         study\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    491\u001b[0m         func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[0;32m    492\u001b[0m         n_trials\u001b[38;5;241m=\u001b[39mn_trials,\n\u001b[0;32m    493\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[0;32m    494\u001b[0m         n_jobs\u001b[38;5;241m=\u001b[39mn_jobs,\n\u001b[0;32m    495\u001b[0m         catch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mtuple\u001b[39m(catch) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(catch, Iterable) \u001b[38;5;28;01melse\u001b[39;00m (catch,),\n\u001b[0;32m    496\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[0;32m    497\u001b[0m         gc_after_trial\u001b[38;5;241m=\u001b[39mgc_after_trial,\n\u001b[0;32m    498\u001b[0m         show_progress_bar\u001b[38;5;241m=\u001b[39mshow_progress_bar,\n\u001b[0;32m    499\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\optuna\\study\\_optimize.py:64\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 64\u001b[0m         _optimize_sequential(\n\u001b[0;32m     65\u001b[0m             study,\n\u001b[0;32m     66\u001b[0m             func,\n\u001b[0;32m     67\u001b[0m             n_trials,\n\u001b[0;32m     68\u001b[0m             timeout,\n\u001b[0;32m     69\u001b[0m             catch,\n\u001b[0;32m     70\u001b[0m             callbacks,\n\u001b[0;32m     71\u001b[0m             gc_after_trial,\n\u001b[0;32m     72\u001b[0m             reseed_sampler_rng\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     73\u001b[0m             time_start\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     74\u001b[0m             progress_bar\u001b[38;5;241m=\u001b[39mprogress_bar,\n\u001b[0;32m     75\u001b[0m         )\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     77\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\optuna\\study\\_optimize.py:161\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    158\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 161\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m _run_trial(study, func, catch)\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    167\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\optuna\\study\\_optimize.py:253\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    246\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    249\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[0;32m    250\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    252\u001b[0m ):\n\u001b[1;32m--> 253\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[0;32m    254\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\optuna\\study\\_optimize.py:201\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[0;32m    200\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 201\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m func(trial)\n\u001b[0;32m    202\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    203\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    204\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "File \u001b[1;32m~\\kaggle\\calorie\\src\\models\\lgbm\\lgbm_optuna_optimizer.py:65\u001b[0m, in \u001b[0;36mcreate_objective.<locals>.objective\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m     56\u001b[0m params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_depth\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_depth\u001b[39m\u001b[38;5;124m\"\u001b[39m], min_required_depth)\n\u001b[0;32m     58\u001b[0m trainer \u001b[38;5;241m=\u001b[39m LGBMCVTrainer(\n\u001b[0;32m     59\u001b[0m     params\u001b[38;5;241m=\u001b[39mparams,\n\u001b[0;32m     60\u001b[0m     n_splits\u001b[38;5;241m=\u001b[39mn_splits,\n\u001b[0;32m     61\u001b[0m     early_stopping_rounds\u001b[38;5;241m=\u001b[39mearly_stopping_rounds,\n\u001b[0;32m     62\u001b[0m     cat_cols\u001b[38;5;241m=\u001b[39mcat_cols\n\u001b[0;32m     63\u001b[0m )\n\u001b[1;32m---> 65\u001b[0m trainer\u001b[38;5;241m.\u001b[39mfit_one_fold(tr_df, fold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     67\u001b[0m best_iteration \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mfold_models[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mbest_iteration\n\u001b[0;32m     68\u001b[0m trial\u001b[38;5;241m.\u001b[39mset_user_attr(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest_iteration\u001b[39m\u001b[38;5;124m\"\u001b[39m, best_iteration)\n",
      "File \u001b[1;32m~\\kaggle\\calorie\\src\\models\\lgbm\\lgbm_cv_trainer.py:338\u001b[0m, in \u001b[0;36mLGBMCVTrainer.fit_one_fold\u001b[1;34m(self, tr_df, fold)\u001b[0m\n\u001b[0;32m    334\u001b[0m dvalid \u001b[38;5;241m=\u001b[39m lgb\u001b[38;5;241m.\u001b[39mDataset(X_val, label\u001b[38;5;241m=\u001b[39my_val, reference\u001b[38;5;241m=\u001b[39mdtrain)\n\u001b[0;32m    336\u001b[0m evals_result \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m--> 338\u001b[0m model \u001b[38;5;241m=\u001b[39m lgb\u001b[38;5;241m.\u001b[39mtrain(\n\u001b[0;32m    339\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams,\n\u001b[0;32m    340\u001b[0m     dtrain,\n\u001b[0;32m    341\u001b[0m     num_boost_round\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20000\u001b[39m,\n\u001b[0;32m    342\u001b[0m     valid_sets\u001b[38;5;241m=\u001b[39m[dtrain, dvalid],\n\u001b[0;32m    343\u001b[0m     valid_names\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    344\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m    345\u001b[0m         lgb\u001b[38;5;241m.\u001b[39mearly_stopping(stopping_rounds\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mearly_stopping_rounds),\n\u001b[0;32m    346\u001b[0m         lgb\u001b[38;5;241m.\u001b[39mrecord_evaluation(evals_result),\n\u001b[0;32m    347\u001b[0m         lgb\u001b[38;5;241m.\u001b[39mlog_evaluation(period\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[0;32m    348\u001b[0m     ]\n\u001b[0;32m    349\u001b[0m )\n\u001b[0;32m    351\u001b[0m end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m    352\u001b[0m duration \u001b[38;5;241m=\u001b[39m end \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\lightgbm\\engine.py:328\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(params, train_set, num_boost_round, valid_sets, valid_names, feval, init_model, keep_training_booster, callbacks)\u001b[0m\n\u001b[0;32m    326\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m valid_sets \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    327\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_valid_contain_train:\n\u001b[1;32m--> 328\u001b[0m         evaluation_result_list\u001b[38;5;241m.\u001b[39mextend(booster\u001b[38;5;241m.\u001b[39meval_train(feval))\n\u001b[0;32m    329\u001b[0m     evaluation_result_list\u001b[38;5;241m.\u001b[39mextend(booster\u001b[38;5;241m.\u001b[39meval_valid(feval))\n\u001b[0;32m    330\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\lightgbm\\basic.py:4408\u001b[0m, in \u001b[0;36mBooster.eval_train\u001b[1;34m(self, feval)\u001b[0m\n\u001b[0;32m   4376\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21meval_train\u001b[39m(\n\u001b[0;32m   4377\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4378\u001b[0m     feval: Optional[Union[_LGBM_CustomEvalFunction, List[_LGBM_CustomEvalFunction]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   4379\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[_LGBM_BoosterEvalMethodResultType]:\n\u001b[0;32m   4380\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Evaluate for training data.\u001b[39;00m\n\u001b[0;32m   4381\u001b[0m \n\u001b[0;32m   4382\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4406\u001b[0m \u001b[38;5;124;03m        List with (train_dataset_name, eval_name, eval_result, is_higher_better) tuples.\u001b[39;00m\n\u001b[0;32m   4407\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4408\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__inner_eval(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_data_name, \u001b[38;5;241m0\u001b[39m, feval)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\lightgbm\\basic.py:5185\u001b[0m, in \u001b[0;36mBooster.__inner_eval\u001b[1;34m(self, data_name, data_idx, feval)\u001b[0m\n\u001b[0;32m   5182\u001b[0m result \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__num_inner_eval, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat64)\n\u001b[0;32m   5183\u001b[0m tmp_out_len \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mc_int(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m   5184\u001b[0m _safe_call(\n\u001b[1;32m-> 5185\u001b[0m     _LIB\u001b[38;5;241m.\u001b[39mLGBM_BoosterGetEval(\n\u001b[0;32m   5186\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle,\n\u001b[0;32m   5187\u001b[0m         ctypes\u001b[38;5;241m.\u001b[39mc_int(data_idx),\n\u001b[0;32m   5188\u001b[0m         ctypes\u001b[38;5;241m.\u001b[39mbyref(tmp_out_len),\n\u001b[0;32m   5189\u001b[0m         result\u001b[38;5;241m.\u001b[39mctypes\u001b[38;5;241m.\u001b[39mdata_as(ctypes\u001b[38;5;241m.\u001b[39mPOINTER(ctypes\u001b[38;5;241m.\u001b[39mc_double)),\n\u001b[0;32m   5190\u001b[0m     )\n\u001b[0;32m   5191\u001b[0m )\n\u001b[0;32m   5192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tmp_out_len\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__num_inner_eval:\n\u001b[0;32m   5193\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWrong length of eval results\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# tr_df1のtuning\n",
    "importlib.reload(cv)\n",
    "importlib.reload(op)\n",
    "objective = op.create_objective(\n",
    "    tr_df1,\n",
    "    early_stopping_rounds=500,\n",
    "    cat_cols=cat_cols,\n",
    "    n_jobs=25\n",
    ")\n",
    "\n",
    "random_sampler = optuna.samplers.RandomSampler(seed=42)\n",
    "\n",
    "study = op.run_optuna_search(\n",
    "    objective,\n",
    "    n_trials=10,\n",
    "    n_jobs=1,\n",
    "    study_name=\"lgb_v1.1\",\n",
    "    storage=url,\n",
    "    sampler=random_sampler\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7b05095-2b2c-4218-8daa-a2c1af0c3239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 00:18:46\n",
      "Successfully saved test predictions to ../artifacts/test_preds/full/test_full_16.npy\n"
     ]
    }
   ],
   "source": [
    "# 16のfull train\n",
    "params = {\n",
    "    \"max_depth\": 8,\n",
    "    \"num_leaves\": 621,\n",
    "    \"min_child_samples\": 9729,\n",
    "    \"min_split_gain\": 0.0008793022734111571,\n",
    "    \"feature_fraction\": 0.3514478163144178,\n",
    "    \"bagging_fraction\": 0.8842473071543802,\n",
    "    \"bagging_freq\": 2,\n",
    "    \"lambda_l1\": 0.00026003894788305436,\n",
    "    \"lambda_l2\": 2.112199050376843\n",
    "}\n",
    "\n",
    "trainer = cv.LGBCVTrainer(params=params, cat_cols=cat_cols)\n",
    "trainer.full_train(tr_df4, test_df4, 851, 16)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (RAPIDS)",
   "language": "python",
   "name": "rapids-23.12"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
